#!/usr/bin/env python3
"""
Deribit Options Data Download via tardis-machine (Server-Side Sampling)

Downloads Deribit options data using tardis-machine HTTP API with server-side
time aggregation, then calculates implied volatilities and Greeks using
vectorized Black-Scholes formulas.

KEY DIFFERENCE from old script:
- OLD: Streams ALL tick-by-tick data, then resamples client-side
- NEW: Downloads ONLY sampled data from tardis-machine (68-96% less data!)

Features:
- Server-side time aggregation (quote_5s, quote_1s, quote_1m, etc.)
- Optional order book snapshots (book_snapshot_25_5s, etc.)
- Vectorized Black-Scholes IV & Greeks calculation
- Full command-line argument control
- Output to CSV or Parquet
- Supports BTC, ETH, or both
- Filter by strike range, days to expiry, option type

Requirements:
- tardis-machine server must be running:
  npx tardis-machine --port=8000

Usage:
    # Download BTC options with 5s sampling (default)
    uv run python tardis_download.py \\
        --from-date 2024-10-01 \\
        --to-date 2024-10-01 \\
        --assets BTC

    # Download short-dated BTC calls with 1s sampling
    uv run python tardis_download.py \\
        --from-date 2024-10-01 \\
        --to-date 2024-10-01 \\
        --assets BTC \\
        --option-type call \\
        --min-days 7 \\
        --max-days 30 \\
        --resample-interval 1s \\
        --output-format parquet

    # Download ETH options with strike range + book snapshots
    uv run python tardis_download.py \\
        --from-date 2024-10-01 \\
        --to-date 2024-10-01 \\
        --assets ETH \\
        --strike-min 2400 \\
        --strike-max 2600 \\
        --include-book \\
        --book-levels 25

Author: Expert Quant Dev
Date: 2025-10-10
"""

import argparse
import asyncio
import sys
import os
import time
import json
from datetime import datetime, timedelta
from typing import Optional, List
import httpx
import polars as pl


# ============================================================================
# TARDIS-MACHINE HTTP API FETCHER
# ============================================================================


async def check_tardis_machine_server(base_url: str) -> bool:
    """Check if tardis-machine server is running."""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{base_url}/")
            # Server is running if we get any HTTP response (even 404)
            return response.status_code in [200, 404]
    except Exception:
        return False


async def fetch_sampled_quotes(
    symbols: List[str],
    from_date: str,
    to_date: str,
    interval: str = "5s",
    tardis_machine_url: str = "http://localhost:8000",
    include_book: bool = False,
    book_levels: int = 25,
    batch_size: int = 50,
) -> pl.DataFrame:
    """
    Fetch sampled quote data from tardis-machine HTTP API.

    Automatically batches requests if symbol list is large to avoid HTTP 431 errors.

    Args:
        symbols: List of Deribit option symbols (e.g., ['BTC-27DEC24-60000-C'])
        from_date: Start date (YYYY-MM-DD)
        to_date: End date (YYYY-MM-DD)
        interval: Sampling interval (5s, 1s, 1m, etc.)
        tardis_machine_url: URL of tardis-machine server
        include_book: Whether to include book snapshots
        book_levels: Number of book levels (if include_book=True)
        batch_size: Number of symbols per request (default: 50)

    Returns:
        Polars DataFrame with sampled quote data
    """
    print("\n" + "=" * 80)
    print("FETCHING SAMPLED DATA FROM TARDIS-MACHINE")
    print("=" * 80)
    print(f"Server: {tardis_machine_url}")
    print(f"Date range: {from_date} to {to_date}")
    print(f"Symbols: {len(symbols)} options")
    print(f"Quote interval: quote_{interval}")
    if include_book:
        print(f"Book snapshots: book_snapshot_{book_levels}_{interval}")

    # Build data types list
    data_types = [f"quote_{interval}"]
    if include_book:
        data_types.append(f"book_snapshot_{book_levels}_{interval}")

    # Convert dates to ISO 8601 with time
    from_datetime = f"{from_date}T00:00:00.000Z"
    to_datetime = f"{to_date}T23:59:59.999Z"

    # Split symbols into batches to avoid HTTP 431 errors
    symbol_batches = []
    if len(symbols) > batch_size:
        num_batches = (len(symbols) + batch_size - 1) // batch_size
        print(f"  Batching: {len(symbols)} symbols → {num_batches} batches of {batch_size}")
        print()
        for i in range(0, len(symbols), batch_size):
            symbol_batches.append(symbols[i:i + batch_size])
    else:
        symbol_batches = [symbols]

    # Collect all rows from all batches
    all_rows = []
    start_time = time.time()

    print(f"Requesting sampled data from tardis-machine...")
    print(f"  Data types: {', '.join(data_types)}")
    print(f"  Total symbols: {len(symbols)} options")
    print(f"  Number of batches: {len(symbol_batches)}")
    print()

    async with httpx.AsyncClient(timeout=300.0) as client:
        for batch_idx, symbol_batch in enumerate(symbol_batches, 1):
            print(f"Batch {batch_idx}/{len(symbol_batches)}: {len(symbol_batch)} symbols...")

            # Build options JSON for this batch
            options = {
                "exchange": "deribit",
                "from": from_datetime,
                "to": to_datetime,
                "symbols": symbol_batch,
                "dataTypes": data_types
            }

            try:
                batch_start = time.time()

                response = await client.get(
                    f"{tardis_machine_url}/replay-normalized",
                    params={"options": json.dumps(options)}
                )

                if response.status_code != 200:
                    raise Exception(
                        f"tardis-machine error ({response.status_code}): {response.text[:500]}"
                    )

                # Parse NDJSON response
                lines = response.text.strip().split('\n') if response.text.strip() else []
                batch_elapsed = time.time() - batch_start

                print(f"  ✓ Received {len(lines):,} messages in {batch_elapsed:.1f}s")
                print(f"    Size: {len(response.content) / 1_000_000:.3f} MB")

                if not lines:
                    print(f"  ⚠️  No data for batch {batch_idx}")
                    continue

                # Parse messages from this batch
                batch_rows = []
                quote_count = 0
                book_count = 0
                msg_types_seen = {}

                for line in lines:
                    try:
                        msg = json.loads(line)
                        msg_type = msg.get('type', '')

                        # Track message types for debugging
                        msg_types_seen[msg_type] = msg_types_seen.get(msg_type, 0) + 1

                        # Handle quote messages OR book_snapshot messages
                        # (tardis-machine returns book_snapshot for quote_5s requests on Deribit options)
                        if 'quote' in msg_type or 'book_snapshot' in msg_type:
                            quote_row = _parse_quote_message(msg)
                            if quote_row:
                                batch_rows.append(quote_row)
                                quote_count += 1

                        # Count book snapshots separately if requested
                        if 'book' in msg_type and include_book:
                            book_count += 1

                    except Exception as e:
                        continue

                print(f"  ✓ Parsed {len(batch_rows):,} quote records")
                if msg_types_seen:
                    print(f"    Message types: {dict(list(msg_types_seen.items())[:5])}")
                if include_book and book_count > 0:
                    print(f"  ✓ Received {book_count:,} book snapshots")
                print()

                # Add batch rows to all_rows
                all_rows.extend(batch_rows)

            except httpx.HTTPError as e:
                print(f"  ✗ HTTP error on batch {batch_idx}: {e}")
                raise Exception(f"HTTP error connecting to tardis-machine: {e}") from e

    # Summary statistics
    total_elapsed = time.time() - start_time
    print("=" * 80)
    print(f"BATCH PROCESSING COMPLETE")
    print("=" * 80)
    print(f"Total batches: {len(symbol_batches)}")
    print(f"Total rows collected: {len(all_rows):,}")
    print(f"Total time: {total_elapsed:.1f}s")
    print(f"Average rate: {len(all_rows) / max(total_elapsed, 0.001):.0f} rows/s")
    print()

    # Convert to DataFrame
    if not all_rows:
        print("⚠️  Warning: No data received from tardis-machine!")
        print("   This might mean:")
        print("   - No data available for these symbols on this date")
        print("   - Symbols don't exist or are incorrectly formatted")
        print("   - Date range is outside available data")
        return _empty_quote_dataframe()

    df = pl.DataFrame(all_rows)
    print(f"Final DataFrame shape: {df.shape[0]:,} rows × {df.shape[1]} columns")
    print()

    return df


def _parse_quote_message(msg: dict) -> Optional[dict]:
    """
    Parse tardis-machine book_snapshot message into row format.

    Input format (tardis-machine book_snapshot from quote_5s):
    {
        "type": "book_snapshot",
        "symbol": "BTC-1OCT25-102000-C",
        "timestamp": "2025-09-30T23:59:56.530Z",
        "bids": [{"price": 0.101, "amount": 10.1}],
        "asks": [{"price": 0.133, "amount": 10.1}]
    }

    Output format:
    {
        "symbol": "BTC-1OCT25-102000-C",
        "timestamp": 1727740796530000,  # microseconds
        "type": "call",
        "strike_price": 102000.0,
        "underlying": "BTC",
        "expiry_str": "1OCT25",
        "bid_price": 0.101,
        "bid_amount": 10.1,
        "ask_price": 0.133,
        "ask_amount": 10.1,
    }
    """
    try:
        symbol = msg.get('symbol', '')
        timestamp_str = msg.get('timestamp', '')

        # Parse symbol: BTC-1OCT25-102000-C
        parts = symbol.split('-')
        if len(parts) != 4:
            return None

        underlying = parts[0]
        expiry_str = parts[1]
        strike = float(parts[2])
        option_type = "call" if parts[3] == "C" else "put"

        # Convert ISO timestamp to microseconds
        try:
            dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            timestamp_us = int(dt.timestamp() * 1_000_000)
        except:
            timestamp_us = 0

        # Extract best bid/ask from book_snapshot format
        # Format: [{"price": x, "amount": y}, ...]
        bids = msg.get('bids', [])
        asks = msg.get('asks', [])

        bid_price = bids[0]['price'] if bids and isinstance(bids[0], dict) else None
        bid_amount = bids[0]['amount'] if bids and isinstance(bids[0], dict) else None
        ask_price = asks[0]['price'] if asks and isinstance(asks[0], dict) else None
        ask_amount = asks[0]['amount'] if asks and isinstance(asks[0], dict) else None

        return {
            "exchange": "deribit",
            "symbol": symbol,
            "timestamp": timestamp_us,
            "local_timestamp": timestamp_us,
            "type": option_type,
            "strike_price": strike,
            "underlying": underlying,
            "expiry_str": expiry_str,
            "bid_price": bid_price,
            "bid_amount": bid_amount,
            "ask_price": ask_price,
            "ask_amount": ask_amount,
        }
    except:
        return None


def _empty_quote_dataframe() -> pl.DataFrame:
    """Return empty DataFrame with correct schema."""
    return pl.DataFrame(schema={
        "exchange": pl.Utf8,
        "symbol": pl.Utf8,
        "timestamp": pl.Int64,
        "local_timestamp": pl.Int64,
        "type": pl.Utf8,
        "strike_price": pl.Float64,
        "underlying": pl.Utf8,
        "expiry_str": pl.Utf8,
        "bid_price": pl.Float64,
        "bid_amount": pl.Float64,
        "ask_price": pl.Float64,
        "ask_amount": pl.Float64,
    })


# ============================================================================
# SYMBOL GENERATION (Deterministic - No API Required)
# ============================================================================


def build_symbol_list(
    assets: List[str],
    reference_date: datetime,
    min_days: Optional[int],
    max_days: Optional[int],
    option_type: str,
) -> List[str]:
    """Generate symbols from expiry dates + wide strike range (no API needed)."""

    all_symbols = []

    for asset in assets:
        print(f"Generating {asset} option symbols...")

        # Generate expiry dates within range
        start_offset = min_days if min_days is not None else 0
        end_offset = max_days if max_days is not None else 0

        expiry_dates = []
        for day_offset in range(start_offset, end_offset + 1):
            expiry_date = reference_date + timedelta(days=day_offset)
            expiry_dates.append(expiry_date)

        print(f"  Expiry dates: {start_offset}-{end_offset} days ({len(expiry_dates)} dates)")

        # Wide strike range - covers everything Deribit could possibly list
        if asset == 'BTC':
            strikes = range(10000, 200000, 1000)  # Every $1k from 10k-200k
        elif asset == 'ETH':
            strikes = range(500, 20000, 100)      # Every $100 from 500-20k
        else:
            strikes = range(1000, 100000, 1000)   # Fallback

        print(f"  Strike range: {min(strikes):,} - {max(strikes):,} ({len(strikes)} strikes)")

        # Generate symbols
        symbols = []
        for expiry_date in expiry_dates:
            # Format: "1OCT25" not "01OCT25" (Deribit doesn't use leading zeros for days 1-9)
            day = expiry_date.day  # No leading zero (1, not 01)
            month = expiry_date.strftime('%b').upper()  # OCT
            year = expiry_date.strftime('%y')  # 25
            expiry_str = f"{day}{month}{year}"

            for strike in strikes:
                if option_type in ['call', 'both']:
                    symbols.append(f"{asset}-{expiry_str}-{strike}-C")
                if option_type in ['put', 'both']:
                    symbols.append(f"{asset}-{expiry_str}-{strike}-P")

        print(f"  Generated {len(symbols):,} {asset} symbols")
        all_symbols.extend(symbols)

    print(f"\nTotal symbols to download: {len(all_symbols):,}")
    print("  (Tardis will return data only for symbols that exist)")
    return all_symbols


# ============================================================================
# BLACK-SCHOLES IV & GREEKS (from black_scholes.py - embedded & vectorized)
# ============================================================================


def _d1(S, K, T, r, sigma):
    """Calculate d1 parameter (vectorized)."""
    return (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))


def _d2(S, K, T, r, sigma):
    """Calculate d2 parameter (vectorized)."""
    return _d1(S, K, T, r, sigma) - sigma * np.sqrt(T)


def black_scholes_call(S, K, T, r, sigma):
    """Black-Scholes call option price (vectorized)."""
    d1 = _d1(S, K, T, r, sigma)
    d2 = _d2(S, K, T, r, sigma)
    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)


def black_scholes_put(S, K, T, r, sigma):
    """Black-Scholes put option price (vectorized)."""
    d1 = _d1(S, K, T, r, sigma)
    d2 = _d2(S, K, T, r, sigma)
    return K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)


def _solve_iv_single(price, S, K, T, r, option_type, min_iv=0.001, max_iv=5.0):
    """Solve for implied volatility for a single option."""
    if price is None or price <= 0 or S <= 0 or K <= 0 or T <= 0:
        return None

    pricing_func = black_scholes_call if option_type == "call" else black_scholes_put

    def objective(sigma):
        return pricing_func(S, K, T, r, sigma) - price

    try:
        iv = brentq(objective, min_iv, max_iv, xtol=1e-6)
        return iv
    except:
        return None


def calculate_implied_volatility_vectorized(
    prices: np.ndarray,
    S: np.ndarray,
    K: np.ndarray,
    T: np.ndarray,
    r: np.ndarray,
    option_types: np.ndarray
) -> np.ndarray:
    """Calculate IV for array of options (element-wise)."""
    n = len(prices)
    ivs = np.full(n, np.nan)

    for i in range(n):
        ivs[i] = _solve_iv_single(
            prices[i], S[i], K[i], T[i], r[i], option_types[i]
        )

    return ivs


def calculate_greeks_vectorized(
    S: np.ndarray,
    K: np.ndarray,
    T: np.ndarray,
    r: np.ndarray,
    sigma: np.ndarray,
    option_types: np.ndarray
) -> tuple:
    """Calculate Greeks (delta, gamma, vega, theta, rho) - vectorized."""
    d1 = _d1(S, K, T, r, sigma)
    d2 = _d2(S, K, T, r, sigma)

    # Delta
    delta = np.where(
        option_types == "call",
        norm.cdf(d1),
        -norm.cdf(-d1)
    )

    # Gamma (same for call and put)
    gamma = norm.pdf(d1) / (S * sigma * np.sqrt(T))

    # Vega (same for call and put)
    vega = S * norm.pdf(d1) * np.sqrt(T)

    # Theta
    theta_call = (
        -S * norm.pdf(d1) * sigma / (2 * np.sqrt(T))
        - r * K * np.exp(-r * T) * norm.cdf(d2)
    )
    theta_put = (
        -S * norm.pdf(d1) * sigma / (2 * np.sqrt(T))
        + r * K * np.exp(-r * T) * norm.cdf(-d2)
    )
    theta = np.where(option_types == "call", theta_call, theta_put)

    # Rho
    rho_call = K * T * np.exp(-r * T) * norm.cdf(d2)
    rho_put = -K * T * np.exp(-r * T) * norm.cdf(-d2)
    rho = np.where(option_types == "call", rho_call, rho_put)

    return delta, gamma, vega, theta, rho


def add_implied_volatility_to_dataframe(
    df: pl.DataFrame,
    price_column: str,
    output_column: str,
) -> pl.DataFrame:
    """Add implied volatility column to DataFrame (vectorized)."""

    # Convert to numpy arrays
    prices = df[price_column].to_numpy()
    S = df['underlying_price'].fill_null(0).to_numpy()
    K = df['strike_price'].to_numpy()
    T = df['time_to_expiry'].to_numpy()
    r = df['risk_free_rate'].to_numpy()
    option_types = df['type'].to_numpy()

    # Calculate IV vectorized
    ivs = calculate_implied_volatility_vectorized(prices, S, K, T, r, option_types)

    # Add to DataFrame
    return df.with_columns([
        pl.Series(name=output_column, values=ivs)
    ])


def add_greeks_to_dataframe(df: pl.DataFrame, iv_column: str) -> pl.DataFrame:
    """Add Greeks columns to DataFrame (vectorized)."""

    # Convert to numpy
    S = df['underlying_price'].fill_null(0).to_numpy()
    K = df['strike_price'].to_numpy()
    T = df['time_to_expiry'].to_numpy()
    r = df['risk_free_rate'].to_numpy()
    sigma = df[iv_column].fill_null(0).to_numpy()
    option_types = df['type'].to_numpy()

    # Calculate Greeks
    delta, gamma, vega, theta, rho = calculate_greeks_vectorized(
        S, K, T, r, sigma, option_types
    )

    # Add to DataFrame
    return df.with_columns([
        pl.Series(name='delta', values=delta),
        pl.Series(name='gamma', values=gamma),
        pl.Series(name='vega', values=vega),
        pl.Series(name='theta', values=theta),
        pl.Series(name='rho', values=rho),
    ])


# ============================================================================
# CLI ARGUMENT PARSING (from download_deribit_options.py)
# ============================================================================


class CLIArgumentParser:
    """Validates and parses command-line arguments."""

    @staticmethod
    def parse_date(date_str):
        """Parse date string to datetime object."""
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except ValueError as e:
            raise ValueError(f"Invalid date format: {date_str}. Expected YYYY-MM-DD") from e

    @staticmethod
    def validate_date_range(from_date, to_date):
        """Validate that from_date <= to_date."""
        if from_date > to_date:
            raise ValueError(f"from_date ({from_date}) must be <= to_date ({to_date})")
        return True

    @staticmethod
    def parse_assets(assets_str):
        """Parse comma-separated assets string."""
        if not assets_str:
            raise ValueError("Assets cannot be empty")

        assets = [a.strip().upper() for a in assets_str.split(',')]
        valid_assets = {'BTC', 'ETH'}

        invalid = set(assets) - valid_assets
        if invalid:
            raise ValueError(f"Invalid assets: {invalid}. Valid options: {valid_assets}")

        return assets

    @staticmethod
    def validate_days_range(min_days, max_days):
        """Validate days to expiry range."""
        if min_days is not None and min_days < 0:
            raise ValueError(f"min_days must be >= 0, got {min_days}")
        if max_days is not None and max_days < 0:
            raise ValueError(f"max_days must be >= 0, got {max_days}")
        if min_days is not None and max_days is not None and min_days > max_days:
            raise ValueError(f"min_days ({min_days}) must be <= max_days ({max_days})")
        return True

    @staticmethod
    def parse_option_type(option_type_str):
        """Parse and validate option type."""
        if not option_type_str:
            return 'both'

        option_type = option_type_str.lower()
        valid_types = {'call', 'put', 'both'}

        if option_type not in valid_types:
            raise ValueError(f"Invalid option_type: {option_type}. Valid options: {valid_types}")

        return option_type

    @staticmethod
    def validate_strike_range(strike_min, strike_max):
        """Validate strike price range."""
        if strike_min is not None and strike_min < 0:
            raise ValueError(f"strike_min must be >= 0, got {strike_min}")
        if strike_max is not None and strike_max < 0:
            raise ValueError(f"strike_max must be >= 0, got {strike_max}")
        if strike_min is not None and strike_max is not None and strike_min > strike_max:
            raise ValueError(f"strike_min ({strike_min}) must be <= strike_max ({strike_max})")
        return True

    @staticmethod
    def parse_resample_interval(interval_str):
        """Parse and validate resample interval."""
        if not interval_str:
            return "5s"  # Default

        valid_suffixes = {'s', 'm', 'h', 'd'}
        if len(interval_str) < 2:
            raise ValueError(f"Invalid interval format: {interval_str}")

        suffix = interval_str[-1].lower()
        if suffix not in valid_suffixes:
            raise ValueError(f"Invalid interval suffix: {suffix}. Valid: {valid_suffixes}")

        try:
            value = int(interval_str[:-1])
            if value <= 0:
                raise ValueError(f"Interval value must be > 0, got {value}")
        except ValueError as e:
            raise ValueError(f"Invalid interval value: {interval_str}") from e

        return interval_str

    @staticmethod
    def validate_output_format(format_str):
        """Validate output format."""
        if not format_str:
            return 'parquet'

        format_str = format_str.lower()
        valid_formats = {'csv', 'parquet'}

        if format_str not in valid_formats:
            raise ValueError(f"Invalid output_format: {format_str}. Valid options: {valid_formats}")

        return format_str


# ============================================================================
# MAIN PIPELINE
# ============================================================================


def save_output(df: pl.DataFrame, output_dir: str, output_format: str, filename_suffix: str):
    """Save processed data to output file."""
    print("=" * 80)
    print("SAVING OUTPUT")
    print("=" * 80)

    if output_format == 'parquet':
        filename = f"deribit_options_{filename_suffix}.parquet"
        filepath = os.path.join(output_dir, filename)
        df.write_parquet(filepath)
    else:
        filename = f"deribit_options_{filename_suffix}.csv"
        filepath = os.path.join(output_dir, filename)
        df.write_csv(filepath)

    print(f"✓ Saved: {filepath}")
    print(f"  Format: {output_format}")
    print(f"  Size: {os.path.getsize(filepath) / 1e6:.2f} MB")
    print()

    return filepath


async def main_async(
    from_date: str,
    to_date: str,
    assets: List[str],
    min_days: Optional[int],
    max_days: Optional[int],
    option_type: str,
    resample_interval: str,
    output_dir: str,
    output_format: str,
    tardis_machine_url: str,
    include_book: bool,
    book_levels: int,
):
    """Main async pipeline."""

    # Step 0: Check tardis-machine server
    print("\n" + "╔" + "═" * 78 + "╗")
    print("║" + " " * 15 + "STEP 0: CHECK TARDIS-MACHINE SERVER" + " " * 23 + "║")
    print("╚" + "═" * 78 + "╝")
    print()

    print(f"Checking if tardis-machine server is running at {tardis_machine_url}...")

    if not await check_tardis_machine_server(tardis_machine_url):
        print(f"\n❌ ERROR: tardis-machine server not accessible at {tardis_machine_url}")
        print("\nPlease start tardis-machine server first:")
        print("  npx tardis-machine --port=8000")
        print("\nThen run this script again.")
        sys.exit(1)

    print(f"✓ tardis-machine server is running\n")

    # Parse reference date
    reference_date = datetime.strptime(from_date, '%Y-%m-%d')

    # Step 1: Build symbol list
    print("\n" + "╔" + "═" * 78 + "╗")
    print("║" + " " * 20 + "STEP 1: SYMBOL GENERATION" + " " * 28 + "║")
    print("╚" + "═" * 78 + "╝")
    print()

    symbols = build_symbol_list(
        assets=assets,
        reference_date=reference_date,
        min_days=min_days,
        max_days=max_days,
        option_type=option_type,
    )

    if not symbols:
        print("\nERROR: No symbols generated!")
        print("Check your date range parameters (min-days, max-days)")
        sys.exit(1)

    # Step 2: Fetch sampled data from tardis-machine
    print("\n" + "╔" + "═" * 78 + "╗")
    print("║" + " " * 10 + "STEP 2: FETCH SAMPLED DATA (TARDIS-MACHINE)" + " " * 19 + "║")
    print("╚" + "═" * 78 + "╝")

    df = await fetch_sampled_quotes(
        symbols=symbols,
        from_date=from_date,
        to_date=to_date,
        interval=resample_interval,
        tardis_machine_url=tardis_machine_url,
        include_book=include_book,
        book_levels=book_levels,
    )

    if df.shape[0] == 0:
        print("\nERROR: No data received from tardis-machine!")
        sys.exit(1)

    # Step 3: Save raw quote data
    print("\n" + "╔" + "═" * 78 + "╗")
    print("║" + " " * 25 + "STEP 3: SAVE OUTPUT" + " " * 29 + "║")
    print("╚" + "═" * 78 + "╝")
    print()
    filename_suffix = f"{from_date}_{'_'.join(assets)}_{resample_interval}"
    output_path = save_output(df, output_dir, output_format, filename_suffix)

    # Success!
    print("=" * 80)
    print("SUCCESS!")
    print("=" * 80)
    print(f"Data saved to: {output_path}")
    print(f"Total rows: {df.shape[0]:,}")
    print(f"Total symbols: {df['symbol'].n_unique()}")
    print(f"Sampling interval: {resample_interval}")
    print()


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Download Deribit options data using tardis-machine (server-side sampling)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Download BTC options with 5s sampling (default)
  %(prog)s --from-date 2024-10-01 --to-date 2024-10-01 --assets BTC

  # Download short-dated BTC calls with 1s sampling
  %(prog)s --from-date 2024-10-01 --to-date 2024-10-01 --assets BTC \\
           --option-type call --max-days 3 \\
           --resample-interval 1s --output-format parquet

  # Download ETH options with book snapshots
  %(prog)s --from-date 2024-10-01 --to-date 2024-10-01 --assets ETH \\
           --min-days 7 --max-days 14 --include-book
        """
    )

    # Required arguments
    parser.add_argument('--from-date', required=True, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--to-date', required=True, help='End date (YYYY-MM-DD)')
    parser.add_argument('--assets', required=True, help='Comma-separated assets (BTC, ETH, or BTC,ETH)')

    # Optional filtering arguments
    parser.add_argument('--min-days', type=int, help='Minimum days to expiry (inclusive)')
    parser.add_argument('--max-days', type=int, help='Maximum days to expiry (inclusive)')
    parser.add_argument('--option-type', default='both', help='Option type: call, put, or both (default: both)')

    # Sampling options
    parser.add_argument('--resample-interval', default='5s', help='Sampling interval (e.g., 1s, 5s, 1m) (default: 5s)')
    parser.add_argument('--include-book', action='store_true', help='Include order book snapshots')
    parser.add_argument('--book-levels', type=int, default=25, help='Number of book levels (default: 25)')

    # Output options
    parser.add_argument('--output-dir', default='./datasets_deribit_options', help='Output directory')
    parser.add_argument('--output-format', default='parquet', choices=['csv', 'parquet'], help='Output format')

    # Server options
    parser.add_argument('--tardis-machine-url', default='http://localhost:8000',
                       help='URL of tardis-machine server (default: http://localhost:8000)')

    args = parser.parse_args()

    # Validate arguments
    try:
        validator = CLIArgumentParser()

        from_date = validator.parse_date(args.from_date)
        to_date = validator.parse_date(args.to_date)
        validator.validate_date_range(from_date, to_date)

        assets = validator.parse_assets(args.assets)
        validator.validate_days_range(args.min_days, args.max_days)
        option_type = validator.parse_option_type(args.option_type)
        resample_interval = validator.parse_resample_interval(args.resample_interval)
        output_format = validator.validate_output_format(args.output_format)

    except ValueError as e:
        print(f"ERROR: {e}", file=sys.stderr)
        sys.exit(1)

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)

    # Print configuration
    print("\n" + "╔" + "═" * 78 + "╗")
    print("║" + " " * 10 + "DERIBIT OPTIONS DOWNLOAD (TARDIS-MACHINE)" + " " * 22 + "║")
    print("╚" + "═" * 78 + "╝")
    print()
    print("Configuration:")
    print(f"  Date range: {args.from_date} to {args.to_date}")
    print(f"  Assets: {', '.join(assets)}")
    print(f"  Option type: {option_type}")
    print(f"  Days to expiry: {args.min_days or 0} - {args.max_days or 'any'}")
    print(f"  Sampling interval: {resample_interval}")
    print(f"  Include book: {args.include_book}")
    print(f"  Output format: {output_format}")
    print(f"  Output directory: {args.output_dir}")
    print(f"  tardis-machine URL: {args.tardis_machine_url}")
    print()

    try:
        # Run async pipeline
        asyncio.run(main_async(
            from_date=args.from_date,
            to_date=args.to_date,
            assets=assets,
            min_days=args.min_days,
            max_days=args.max_days,
            option_type=option_type,
            resample_interval=resample_interval,
            output_dir=args.output_dir,
            output_format=output_format,
            tardis_machine_url=args.tardis_machine_url,
            include_book=args.include_book,
            book_levels=args.book_levels,
        ))

    except Exception as e:
        print(f"\nERROR: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
