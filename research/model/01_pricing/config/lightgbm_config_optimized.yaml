# LightGBM Residual Model - Optimized Configuration (63M rows)
#
# This configuration incorporates best practices from comprehensive research
# and analysis to improve Brier score from 7.91% to 10-15% improvement
#
# Key changes from production:
# - Lower learning rate (0.02) with more estimators (1000)
# - Reduced complexity (num_leaves 15, max_depth 7)
# - Increased min_data_in_leaf (50) for better generalization
# - Reduced min_gain_to_split (0.1) to capture subtle patterns
# - More frequent bagging (bagging_freq 1)
#
# ==============================================================================
# MEMORY OPTIMIZATION - LIGHTGBM NATIVE EFFICIENCY
# ==============================================================================
memory:
  # LightGBM can handle more bins efficiently due to histogram optimization
  max_bin: 255  # Keep high for accuracy on 128GB RAM system

  # Thread control - LightGBM handles threading better than XGBoost
  num_threads: 12  # Utilize 12 cores on 128GB EC2 instance

  # Histogram pool size (-1 = auto-managed by LightGBM)
  histogram_pool_size: -1

  # Minimum data per group for categorical features
  min_data_per_group: 100

  # Maximum categories for a categorical feature
  max_cat_threshold: 32

  # Force row-wise histogram building (lower memory but slower)
  force_row_wise: true

  # Enable feature bundling to save memory
  enable_bundle: true

  # CV sample percentage for hyperparameter tuning
  cv_sample_pct: 0.01  # 1% of 63M = 630K rows for quick iterations

# ==============================================================================
# HYPERPARAMETERS - OPTIMIZED BASED ON ANALYSIS
# ==============================================================================
hyperparameters:
  # --------------------------------------------------------------------------
  # Core Parameters
  # --------------------------------------------------------------------------
  # Objective function (keep regression for residual modeling)
  objective: "regression"

  # Evaluation metrics - MSE directly relates to Brier improvement
  metric: ["mse", "rmse", "mae"]

  # Boosting type - standard gradient boosting
  boosting_type: "gbdt"  # Will test GOSS separately

  # Random seed for reproducibility
  seed: 42

  # --------------------------------------------------------------------------
  # Tree Structure Control - OPTIMIZED FOR PROBABILITY CALIBRATION
  # --------------------------------------------------------------------------
  # Number of leaves - REDUCED for smoother probabilities
  num_leaves: 15  # Down from 31 (smoother probability surfaces)

  # Maximum tree depth - SET EXPLICIT LIMIT
  max_depth: 7  # From -1 (unlimited) to prevent overfitting

  # Minimum samples in leaf - INCREASED for better generalization
  min_data_in_leaf: 50  # Up from 20 (prevents overfitting on 63M rows)

  # Minimum sum of hessian in leaf
  min_sum_hessian_in_leaf: 1e-3

  # --------------------------------------------------------------------------
  # Regularization - FINE-TUNED FOR FINANCIAL DATA
  # --------------------------------------------------------------------------
  # L1 regularization - REDUCED
  lambda_l1: 0.5  # Down from 1.0 (less aggressive)

  # L2 regularization - SLIGHTLY REDUCED
  lambda_l2: 15.0  # Down from 20.0 (allow more flexibility)

  # Minimum loss reduction for split - SIGNIFICANTLY REDUCED
  min_gain_to_split: 0.1  # Down from 1.0 (capture subtle patterns)

  # --------------------------------------------------------------------------
  # Sampling Parameters - OPTIMIZED FOR STABILITY
  # --------------------------------------------------------------------------
  # Feature sampling per tree
  feature_fraction: 0.8  # Keep current (good for 59 features)

  # Row sampling per tree - INCREASED
  bagging_fraction: 0.8  # Up from 0.7 (more data per tree)

  # Frequency of bagging - MORE FREQUENT
  bagging_freq: 1  # Down from 5 (bagging every iteration for more randomness)

  # Feature sampling by node
  feature_fraction_bynode: 0.8

  # --------------------------------------------------------------------------
  # Learning Parameters - KEY OPTIMIZATION
  # --------------------------------------------------------------------------
  # Learning rate - REDUCED for gradual convergence
  learning_rate: 0.02  # Down from 0.05 (finer steps, better convergence)

  # Number of boosting iterations - SIGNIFICANTLY INCREASED
  n_estimators: 1000  # Up from 200 (more trees with lower learning rate)

  # Early stopping rounds - INCREASED PATIENCE
  early_stopping_rounds: 50  # Up from 25 (more patience for convergence)

  # --------------------------------------------------------------------------
  # Advanced LightGBM Features
  # --------------------------------------------------------------------------
  # Path smoothing (helps prevent overfitting)
  path_smooth: 0.0  # Keep disabled for now

  # Extra trees (adds randomness like Random Forest)
  extra_trees: false  # Test separately if needed

  # Use two-round loading (memory efficiency)
  two_round: true

  # Handle missing values
  use_missing: true
  zero_as_missing: false

  # Deterministic training (reproducible results)
  deterministic: true

  # Data sample strategy
  data_sample_strategy: "bagging"  # Will test GOSS separately

  # --------------------------------------------------------------------------
  # Categorical Features (if any)
  # --------------------------------------------------------------------------
  # Maximum number of bins for categorical features
  max_cat_to_onehot: 4  # Convert to one-hot if <=4 categories

  # Categorical smoothing (for high-cardinality categoricals)
  cat_smooth: 10.0

  # L2 regularization in categorical split
  cat_l2: 10.0

# ==============================================================================
# VALIDATION STRATEGY
# ==============================================================================
validation:
  use_validation_set: true
  val_split: 0.2  # 80/20 split
  monitor_metrics:
    - "rmse"
    - "mae"
    - "mse"  # Added for Brier score tracking

  # Stratified sampling
  stratified: false  # Keep temporal order for time series

  # Cross-validation settings (for hyperparameter tuning)
  cv_folds: 5
  cv_shuffle: false  # IMPORTANT: Don't shuffle for time series

# ==============================================================================
# GPU SETTINGS (Optional)
# ==============================================================================
use_gpu: false  # Set to true if GPU available
gpu:
  gpu_device_id: 0
  gpu_platform_id: 0
  gpu_use_dp: false  # Use double precision

# ==============================================================================
# OUTPUT SETTINGS
# ==============================================================================
output:
  # Feature importance type
  importance_type: "gain"  # Most informative for understanding model

  # Number of top features to log
  top_features: 30  # Increased to see more feature interactions

  # Save feature importance plot
  save_importance_plot: true

  # Model formats to save
  save_formats:
    - "txt"  # LightGBM native format
    - "json"  # Human-readable format

# ==============================================================================
# EXPECTED IMPROVEMENTS
# ==============================================================================
# Based on comprehensive analysis and research:
# - Current Brier improvement: 7.91%
# - Expected with these optimizations: 10-12%
# - Additional gains possible with:
#   - Binary objective instead of regression: +1-2%
#   - Feature engineering (interactions): +1-2%
#   - Ensemble methods: +1-2%
#
# Key changes driving improvement:
# 1. Lower learning rate + more trees → better convergence
# 2. Reduced complexity → smoother probabilities
# 3. Lower min_gain_to_split → capture subtle patterns
# 4. Increased min_data_in_leaf → better generalization
# 5. More frequent bagging → increased robustness
improvement_targets:
  expected_brier_improvement: 0.10  # 10% minimum target
  expected_rmse: 0.38  # Down from 0.387
  expected_mae: 0.30  # Down from 0.308