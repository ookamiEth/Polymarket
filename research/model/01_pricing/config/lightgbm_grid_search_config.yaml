# LightGBM Grid Search Configuration
# Two-phase exhaustive hyperparameter tuning with W&B tracking
#
# Phase 1: Coarse Grid (32 combinations, 8-10 hours on 63M rows)
# Phase 2: Fine Grid (optional, refine around best config)

# Grid search parameters (will generate all combinations)
grid_search:
  # Boosting algorithm (CRITICAL: test both)
  boosting_type:
    - "gbdt"  # Standard gradient boosting (slower but often better)
    - "goss"  # Gradient-based One-Side Sampling (3.5x faster)

  # Learning rate (inversely related to n_estimators)
  learning_rate:
    - 0.02  # Conservative (more trees needed, smoother convergence)
    - 0.05  # Baseline (balanced speed/quality)

  # Tree complexity (CRITICAL for probability calibration)
  num_leaves:
    - 15  # Simpler trees (smoother probabilities, better generalization)
    - 31  # More complex (captures finer patterns, risk overfitting)

  # Minimum samples per leaf (CRITICAL for financial data)
  min_data_in_leaf:
    - 20   # Lower threshold (more granular splits)
    - 100  # Higher threshold (smoother, better generalization)

  # Minimum gain to split (captures subtle patterns)
  min_gain_to_split:
    - 0.1  # Lower (capture subtle financial microstructure)
    - 1.0  # Higher (only strong patterns)

  # L2 regularization
  lambda_l2:
    - 10.0  # Lower regularization (more flexible)
    - 20.0  # Higher regularization (more conservative)

# Fixed parameters (not tuned, from optimized config)
fixed_params:
  # Core settings
  objective: "regression"
  metric:
    - "mse"
    - "rmse"
    - "mae"
  seed: 42
  verbose: -1

  # Tree structure (fixed)
  max_depth: -1  # No limit (controlled by num_leaves)
  min_sum_hessian_in_leaf: 0.001

  # L1 regularization (fixed)
  lambda_l1: 0.5

  # Sampling (fixed)
  feature_fraction: 0.8
  bagging_fraction: 0.8
  bagging_freq: 1

  # Training
  n_estimators: 1000  # High limit, rely on early stopping
  early_stopping_rounds: 50

  # Memory optimization
  max_bin: 255
  num_threads: 12
  min_data_per_group: 100
  max_cat_threshold: 32
  histogram_pool_size: -1  # Auto

  # Performance
  use_missing: true
  zero_as_missing: false
  two_round: true
  force_row_wise: true
  deterministic: true
  data_sample_strategy: "bagging"
  enable_bundle: true
  max_conflict_rate: 0.0

# GOSS-specific parameters (only used when boosting_type='goss')
goss_params:
  top_rate: 0.2     # Keep top 20% gradients
  other_rate: 0.1   # Sample 10% of remaining
  # Effective sample rate: 28% per iteration

# Dataset configuration
dataset:
  start_date: "2023-01-01"
  end_date: "2024-10-31"
  chunk_months: 6
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  shuffle: true

# Weights & Biases configuration
wandb:
  project: "lightgbm-residual-tuning"
  entity: null  # Set to your W&B username/team if needed
  tags:
    - "grid-search"
    - "phase-1-coarse"
    - "63M-rows"
    - "2-year-data"
  notes: "Phase 1 coarse grid search: 32 combinations on full 63M row dataset"

  # Run naming pattern: {boosting_type}-lr{learning_rate}-leaves{num_leaves}-...
  run_name_template: "{boosting_type}-lr{learning_rate}-leaves{num_leaves}-mindata{min_data_in_leaf}"

# Grid search execution settings
execution:
  checkpoint_every: 5  # Save progress every N trials
  checkpoint_file: "results/grid_search_checkpoint.json"
  results_csv: "results/grid_search_results.csv"
  best_model_path: "results/lightgbm_model_best_grid.txt"
  summary_file: "results/grid_search_summary.txt"

  # Resume from checkpoint if interrupted
  resume_from_checkpoint: true

  # Parallel execution (if multiple GPUs/instances available)
  parallel: false
  n_parallel_jobs: 1

# Optimization objective
optimization:
  metric: "brier_improvement_pct"  # Primary metric to maximize
  direction: "maximize"

  # Secondary metrics to track
  secondary_metrics:
    - "model_brier"
    - "residual_mse"
    - "residual_rmse"
    - "residual_mae"
    - "training_time_minutes"

# Phase 2 (fine-tuning) - uncomment and modify after Phase 1
# fine_grid:
#   # Example: Refine around best config from Phase 1
#   # If Phase 1 best was: gbdt, lr=0.02, leaves=15, min_data=100, min_gain=0.1, l2=15.0
#   boosting_type: ["gbdt"]  # Fix to best
#   learning_rate: [0.015, 0.02, 0.025]  # Refine ±25%
#   num_leaves: [12, 15, 18]  # Refine ±20%
#   min_data_in_leaf: [80, 100, 120]  # Refine ±20%
#   min_gain_to_split: [0.08, 0.1, 0.12]  # Refine ±20%
#   lambda_l2: [12.0, 15.0, 18.0]  # Refine ±20%
