# LightGBM Moderate Configuration
# Purpose: Balanced regularization - use if conservative config underfits
# Target: Train-validation gap < 8% with good predictive performance
# Dataset: 63M rows (full production) / 2.6M rows (pilot)

# Hyperparameters for training
hyperparameters:
  # Core objective and metrics
  objective: regression
  metric:
    - mse      # MSE directly equals Brier improvement for residual models
    - rmse     # Square root of MSE for interpretability
    - mae      # Mean absolute error
  boosting_type: gbdt
  seed: 42

  # Tree complexity control - MODERATE
  num_leaves: 20              # Between conservative (15) and original (31)
  max_depth: 6                # Slightly deeper than conservative (5)
  min_data_in_leaf: 50        # Between conservative (100) and original (20)
  min_sum_hessian_in_leaf: 1e-3

  # Regularization - MODERATE
  lambda_l1: 3.0              # Between conservative (5.0) and original (1.0)
  lambda_l2: 40.0             # Between conservative (50.0) and original (20.0)
  min_gain_to_split: 1.5      # Between conservative (2.0) and original (1.0)

  # Feature and data sampling - MODERATE
  feature_fraction: 0.7       # Between conservative (0.6) and original (0.8)
  bagging_fraction: 0.65      # Between conservative (0.6) and original (0.7)
  bagging_freq: 4             # Between conservative (3) and original (5)

  # Learning parameters - BALANCED
  learning_rate: 0.04         # Between conservative (0.03) and original (0.05)
  n_estimators: 400           # Fewer than conservative (500), more than original (200)
  early_stopping_rounds: 20   # Between conservative (15) and original (25)

# Memory optimization settings
memory:
  max_bin: 63                 # Histogram bins for continuous features
  min_data_per_group: 100     # Minimum data per categorical group
  max_cat_threshold: 32        # Maximum categorical cardinality
  histogram_pool_size: -1      # Auto-configure based on data
  num_threads: 1               # Single thread for memory safety

# Evaluation settings
evaluation:
  eval_frequency: 10           # Evaluate every 10 iterations
  save_best_model: true        # Save model with best validation score

# Use cases for moderate config:
# 1. Conservative config achieves < 5% gap but validation RMSE is too high
# 2. Need more model capacity while maintaining regularization
# 3. Dataset has clear patterns that require slightly complex trees

# Expected outcomes:
# - Train-val gap: < 8%
# - Better validation RMSE than conservative
# - Memory usage: 10-14GB
# - Good balance between bias and variance

# Tuning notes:
# - This is the "Goldilocks" zone for most production cases
# - If train/val converge too quickly, reduce learning_rate
# - If features show high correlation, reduce feature_fraction