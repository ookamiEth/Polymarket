# XGBoost Residual Model - 30GB RAM System Configuration
#
# This configuration is optimized for systems with 30GB RAM
# Uses more aggressive settings for better performance and accuracy
# No need for extreme memory conservation with this much RAM
#
# ==============================================================================
# MEMORY OPTIMIZATION - BALANCED FOR 30GB SYSTEM
# ==============================================================================
memory:
  # Bin settings - Higher for better accuracy with sufficient RAM
  max_bin: 128  # Increased from 32 (better quantization, 4x more bins)

  # Threading - Can use multiple threads with standard DMatrix
  nthread: 4  # Increased from 1 (4x faster training)
  # Note: Set to 1 only if using ExtMemQuantileDMatrix

  # Histogram cache - Default setting is fine with 30GB
  max_cached_hist_node: 65536  # Default (was 8, too conservative)

  # Enable resource-aware memory management
  ref_resource_aware: true

  # CV sample percentage - Can use larger sample with more RAM
  cv_sample_pct: 0.05  # 5% of 63M = 3.15M rows (statistically robust)

# ==============================================================================
# HYPERPARAMETERS - OPTIMIZED FOR ACCURACY WITH 30GB RAM
# ==============================================================================
hyperparameters:
  # --------------------------------------------------------------------------
  # Tree Structure Control
  # --------------------------------------------------------------------------
  # Maximum depth - Can go deeper with more RAM
  max_depth: 6  # Increased from 5 (captures more complex patterns)

  # Minimum samples per leaf - Balanced
  min_child_weight: 10  # Keep moderate to prevent overfitting

  # --------------------------------------------------------------------------
  # Regularization - BALANCED
  # --------------------------------------------------------------------------
  # Minimum loss reduction for split
  gamma: 0.5  # Reduced from 1.0 (less aggressive pruning)

  # L2 regularization - Moderate
  reg_lambda: 10  # Reduced from 20 (less regularization needed)

  # L1 regularization - Light sparsity
  reg_alpha: 0.5  # Reduced from 1.0

  # --------------------------------------------------------------------------
  # Learning & Ensemble
  # --------------------------------------------------------------------------
  # Learning rate - Keep conservative for stability
  learning_rate: 0.05

  # Number of trees with early stopping
  n_estimators: 300  # Increased from 200 (more room for improvement)

  # Early stopping patience
  early_stopping_rounds: 30  # Increased from 25 (more patience)

  # --------------------------------------------------------------------------
  # Sampling - Less aggressive with more RAM
  # --------------------------------------------------------------------------
  # Row sampling per tree
  subsample: 0.8  # Increased from 0.7

  # Feature sampling per tree
  colsample_bytree: 0.8  # Keep same

  # Feature sampling per level
  colsample_bylevel: 0.8  # Added for more control

  # --------------------------------------------------------------------------
  # Other Settings
  # --------------------------------------------------------------------------
  objective: "reg:squarederror"
  eval_metric: ["rmse", "mae"]
  tree_method: "hist"
  grow_policy: "depthwise"
  seed: 42

# ==============================================================================
# VALIDATION STRATEGY
# ==============================================================================
validation:
  use_validation_set: true
  val_split: 0.2  # 80/20 split
  monitor_metrics:
    - "rmse"
    - "mae"

# ==============================================================================
# BATCH PROCESSING SETTINGS
# ==============================================================================
batch_processing:
  # Batch size for predictions and data loading
  batch_size: 5000000  # 5M rows per batch (increased from 2M)

  # Memory threshold for using external memory (rows)
  external_memory_threshold: 25000000  # 25M rows (increased from 10M)

  # Cache directory for external memory files
  cache_dir: "./xgb_cache"

  # Clean cache after training
  clean_cache: true

# ==============================================================================
# EXPECTED OUTCOMES - 30GB SYSTEM
# ==============================================================================
# Memory Usage:
#   - Peak memory: 15-20GB (safe for 30GB RAM)
#   - Batch processing: 5M rows at a time
#   - Can handle up to 25M rows in standard DMatrix
#
# Model Performance:
#   - Better accuracy due to higher max_bin (128 vs 32)
#   - 4x faster training with nthread=4
#   - Expected improvement: 10-15% over baseline
#   - Training time: 20-45 minutes (faster than production config)
#
# Key Improvements over production config:
#   - max_bin: 32 → 128 (4x better quantization)
#   - nthread: 1 → 4 (4x faster training)
#   - max_cached_hist_node: 8 → 65536 (default, not constrained)
#   - batch_size: 2M → 5M (fewer batches needed)
#   - external_memory_threshold: 10M → 25M (use standard DMatrix more)
#   - max_depth: 5 → 6 (capture more patterns)
#   - n_estimators: 200 → 300 (more trees for better fit)