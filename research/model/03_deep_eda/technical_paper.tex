\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{subcaption}
\geometry{margin=1in}
\title{\textbf{Two-Stage Residual Learning for Binary Option Pricing:\\
A Machine Learning Approach to High-Frequency Market Prediction}}
\author{
Binary Options Research Team\\
\texttt{research@eth.bt}
}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}
We present a novel two-stage residual learning framework for pricing binary options on cryptocurrency markets. Our approach combines the Black-Scholes binary option formula as a non-linear baseline with LightGBM-based residual corrections learned from 196 engineered features spanning market microstructure, volatility dynamics, and order book characteristics. Training on 39 million 15-minute BTC/ETH binary options observations (July 2024 - September 2025), we achieve a 5.6\% reduction in Brier score relative to the Black-Scholes baseline (0.1615 → 0.1524). Through K-means regime clustering, we identify five distinct market conditions with heterogeneous model performance ranging from 4.5\% to 24.8\% improvement. We prove that for residual learning on binary outcomes, mean squared error of residuals directly equals Brier score improvement, providing a principled optimization target. Our analysis reveals that the discontinuous payoff structure of binary options necessitates non-linear baseline modeling and regime-specific feature engineering, contrasting with vanilla option approaches. The system demonstrates highest accuracy in at-the-money, low-volatility, medium-expiry regimes (35\% of data) and produces actionable trading signals with 83.5\% win rate at 10\% edge thresholds.
\end{abstract}
\section{Introduction}
Binary options represent a unique derivative structure where payoff is discontinuous: the holder receives a fixed amount if the underlying asset exceeds a strike price at expiration, and zero otherwise. This all-or-nothing payoff contrasts sharply with vanilla options, where payoff varies continuously with the degree of moneyness. The discontinuity introduces severe non-linearity in pricing dynamics, particularly near at-the-money strikes where small price movements create large probability shifts.
Traditional Black-Scholes theory provides an analytical solution for binary option pricing under geometric Brownian motion assumptions \cite{merton1973theory}. However, cryptocurrency markets exhibit significant departures from these idealized conditions: discrete jumps, volatility clustering, fat-tailed return distributions, and complex microstructure effects from high-frequency trading. These violations suggest potential gains from machine learning corrections to theoretical prices.
Recent work has explored machine learning for vanilla option pricing \cite{dugas2009incorporating, horvath2021deep}, but binary options pose distinct challenges:
\begin{enumerate}
\item \textbf{Extreme non-linearity}: Option delta (sensitivity to underlying price) varies 138× across moneyness states, compared to 3-5× for vanilla calls/puts.
\item \textbf{Heteroskedastic errors}: Prediction error variance follows $\text{Var}(\text{error}) = p(1-p)$, creating a funnel pattern where at-the-money predictions have highest uncertainty.
\item \textbf{Regime dependence}: Model performance varies dramatically across market conditions (4.5\% to 24.8\% improvement in our data).
\item \textbf{High-frequency dynamics}: 15-minute expiries require features capturing momentum, jumps, and order flow at second-level granularity.
\end{enumerate}
We address these challenges through a two-stage residual learning architecture:
\textbf{Stage 1}: Compute Black-Scholes binary option baseline probability $P_{\text{BS}} = e^{-rT} \Phi(d_2)$ using realized volatility and risk-free rate estimates.
\textbf{Stage 2}: Train LightGBM gradient boosting model to predict residuals $\epsilon = \text{Outcome} - P_{\text{BS}}$ using 196 engineered features spanning market microstructure, volatility dynamics, momentum, order book depth, funding rates, and cyclical patterns.
\textbf{Final prediction}: $P_{\text{final}} = P_{\text{BS}} + \hat{\epsilon}_{\text{LightGBM}}$
Our key contributions include:
\begin{itemize}
\item \textbf{Theoretical}: Proof that MSE of residuals directly equals Brier score improvement for binary outcome prediction, justifying our loss function choice.
\item \textbf{Empirical}: Analysis of 39M predictions revealing five market regimes with heterogeneous performance characteristics.
\item \textbf{Feature engineering}: Comprehensive study identifying moneyness and momentum features as primary drivers of residual corrections (41\% and 32\% correlation).
\item \textbf{Practical}: Derivation of trading signals achieving 83.5\% win rate at 10\% edge thresholds, with regime-specific position sizing recommendations.
\end{itemize}
\section{Mathematical Framework}
\subsection{Black-Scholes Binary Option Baseline}
The risk-neutral price of a cash-or-nothing binary call option paying \$1 if $S_T > K$ is:
\begin{equation}
P_{\text{BS}}(S, K, r, \sigma, T) = e^{-rT} \Phi(d_2)
\end{equation}
where:
\begin{align}
d_2 &= \frac{\ln(S/K) + (r - \sigma^2/2)T}{\sigma\sqrt{T}} \\
\Phi(\cdot) &= \text{Standard normal CDF} \\
S &= \text{Spot price at time } t \\
K &= \text{Strike price} \\
r &= \text{Risk-free rate} \\
\sigma &= \text{Implied volatility} \\
T &= \text{Time to expiration}
\end{align}
We estimate $\sigma$ from exponentially weighted moving averages of realized volatility over multiple horizons (60s, 300s, 900s, 3600s) and proxy $r$ using blended DeFi lending rates from Aave and Compound.
\subsection{Residual Target Definition}
Define the residual as the difference between actual binary outcome (0 or 1) and baseline prediction:
\begin{equation}
\epsilon = Y - P_{\text{BS}}
\end{equation}
where $Y \in \{0, 1\}$ is the realized option payoff. Our final prediction is:
\begin{equation}
P_{\text{final}} = P_{\text{BS}} + \hat{\epsilon}_{\text{ML}}
\end{equation}
where $\hat{\epsilon}_{\text{ML}}$ is the machine learning correction trained via gradient boosting.
\subsection{Loss Function and Brier Score Connection}
\textbf{Theorem 1}: For binary outcome prediction with residual learning, mean squared error of residuals equals Brier score improvement.
\begin{proof}
The Brier score for baseline model is:
\begin{equation}
\text{Brier}_{\text{BS}} = \mathbb{E}[(Y - P_{\text{BS}})^2] = \mathbb{E}[\epsilon^2]
\end{equation}
The Brier score for final model is:
\begin{align}
\text{Brier}_{\text{final}} &= \mathbb{E}[(Y - P_{\text{final}})^2] \\
&= \mathbb{E}[(Y - (P_{\text{BS}} + \hat{\epsilon}_{\text{ML}}))^2] \\
&= \mathbb{E}[(\epsilon - \hat{\epsilon}_{\text{ML}})^2]
\end{align}
The Brier score improvement is:
\begin{align}
\Delta \text{Brier} &= \text{Brier}_{\text{BS}} - \text{Brier}_{\text{final}} \\
&= \mathbb{E}[\epsilon^2] - \mathbb{E}[(\epsilon - \hat{\epsilon}_{\text{ML}})^2] \\
&= \mathbb{E}[\epsilon^2] - \mathbb{E}[\epsilon^2 - 2\epsilon\hat{\epsilon}_{\text{ML}} + \hat{\epsilon}_{\text{ML}}^2] \\
&= \mathbb{E}[2\epsilon\hat{\epsilon}_{\text{ML}} - \hat{\epsilon}_{\text{ML}}^2]
\end{align}
For perfect residual predictions where $\hat{\epsilon}_{\text{ML}} = \epsilon$:
\begin{align}
\Delta \text{Brier} &= \mathbb{E}[2\epsilon^2 - \epsilon^2] = \mathbb{E}[\epsilon^2] = \text{Brier}_{\text{BS}}
\end{align}
Therefore, minimizing MSE of residual predictions directly maximizes Brier score improvement. This justifies using standard regression loss functions for gradient boosting.
\end{proof}
\textbf{Corollary 1}: The test-set MSE of residuals provides an unbiased estimate of Brier score improvement.
This theoretical connection guides our optimization: we train LightGBM with regression objective (MSE loss) on residual targets, knowing that reductions in residual MSE translate directly to Brier score gains.
\subsection{Heteroskedasticity of Binary Outcomes}
Binary outcome variance depends on the probability:
\begin{equation}
\text{Var}(Y | P) = P(1 - P)
\end{equation}
This creates heteroskedastic errors with maximum variance at $P=0.5$ (at-the-money) and minimum at extremes ($P \to 0$ or $P \to 1$). Our empirical analysis confirms this funnel pattern (Section \ref{sec:error_analysis}), suggesting potential gains from weighted loss functions:
\begin{equation}
\mathcal{L}_{\text{weighted}} = \sum_{i=1}^{N} w_i (\epsilon_i - \hat{\epsilon}_i)^2, \quad w_i = \frac{1}{P_i(1-P_i) + \delta}
\end{equation}
where $\delta$ is a small constant for numerical stability. This weighting scheme is explored in Section \ref{sec:future_work}.
\section{Feature Engineering}
We construct 196 features across 16 categories, engineered specifically for binary option dynamics:
\subsection{Context Features (3)}
\begin{itemize}
\item \textbf{Moneyness}: $(S - K)/K$ — Most important feature (41\% correlation with residuals)
\item \textbf{Time remaining}: Seconds until expiration
\item \textbf{IV staleness}: Time since last implied volatility update
\end{itemize}
\subsection{Realized Volatility (28)}
Exponentially weighted standard deviations over multiple horizons:
\begin{itemize}
\item \textbf{Base RV}: 60s, 300s, 900s, 3600s windows
\item \textbf{RV EMAs}: 5-period and 20-period exponential moving averages
\item \textbf{RV ratios}: Short/long horizon comparisons (e.g., RV\_300s / RV\_3600s)
\item \textbf{RV acceleration}: First and second derivatives
\end{itemize}
\subsection{Microstructure (28)}
Second-level price dynamics:
\begin{itemize}
\item \textbf{Momentum}: Price changes over 60s, 300s, 900s horizons (32\%, 22\%, 19\% residual correlation)
\item \textbf{Range features}: High-low spread, intrabar range
\item \textbf{Reversals}: Mean reversion indicators
\item \textbf{Autocorrelation}: Lag-1 and lag-5 serial correlation
\item \textbf{Hurst exponent}: Measure of long-range dependence
\end{itemize}
\subsection{Order Book (53)}
Bid-ask dynamics at multiple depth levels:
\begin{itemize}
\item \textbf{Level 0 (32 features)}: Best bid/ask prices, sizes, spreads, imbalances
\item \textbf{Level 5 (21 features)}: Aggregated depth to 5 levels, cumulative imbalance
\end{itemize}
\subsection{Derivative Market Features (28)}
\begin{itemize}
\item \textbf{Funding rate (11)}: Perpetual swap funding rates and changes
\item \textbf{Basis (11)}: Spot-futures basis and term structure
\item \textbf{Open interest (6)}: OI changes and OI/volume ratios
\end{itemize}
\subsection{Higher-Order Features (56)}
\begin{itemize}
\item \textbf{Price EMAs (9)}: 5, 10, 20, 50, 100-period moving averages
\item \textbf{Risk metrics (6)}: Sharpe-like ratios, drawdown measures
\item \textbf{GARCH (5)}: Conditional volatility estimates
\item \textbf{Cyclical (3)}: Hour, day-of-week, month effects
\item \textbf{Extremes (8)}: Distance to recent highs/lows, percentile ranks
\item \textbf{Moments (2)}: Skewness and kurtosis of recent returns
\item \textbf{Vol acceleration (1)}: Second derivative of RV
\item \textbf{RV term structure (4)}: Slope and curvature of RV curve across horizons
\end{itemize}
\section{Model Architecture}
\subsection{LightGBM Hyperparameters}
We use gradient boosted decision trees (GBDT) via LightGBM \cite{ke2017lightgbm} with the following configuration:
\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Objective & Regression (MSE) \\
Num leaves & 31 \\
Max depth & -1 (no limit) \\
Learning rate & 0.05 \\
L1 regularization & 1.0 \\
L2 regularization & 20.0 \\
Feature fraction & 0.8 \\
Bagging fraction & 0.7 \\
Bagging frequency & 5 \\
Min data in leaf & 20 \\
\bottomrule
\end{tabular}
LightGBM hyperparameters. Heavy L2 regularization prevents overfitting given 196 features.\\\rule{\textwidth}{0.4pt}
\label{tab:hyperparams}
\end{table}
\subsection{Training Strategy}
\begin{center}\textbf{Algorithm:} 
Two-Stage Residual Learning Pipeline\\\rule{\textwidth}{0.4pt}
\begin{enumerate}
\item Load data with features $X$ and binary outcomes $Y$
\item Compute Black-Scholes baseline: $P_{\text{BS}} = e^{-rT} \Phi(d_2)$
\item Calculate residuals: $\epsilon = Y - P_{\text{BS}}$
\item Split into train/validation/test sets (temporal split)
\item Train LightGBM: $\hat{\epsilon} = f_{\text{GBDT}}(X)$
\item Compute final predictions: $P_{\text{final}} = P_{\text{BS}} + \hat{\epsilon}$
\item Evaluate Brier scores: $\text{Brier}_{\text{BS}}$, $\text{Brier}_{\text{final}}$
\item Report improvement: $\Delta = (\text{Brier}_{\text{BS}} - \text{Brier}_{\text{final}}) / \text{Brier}_{\text{BS}}$
\end{enumerate}
\end{center}
\section{Empirical Results}
\subsection{Dataset}
\begin{itemize}
\item \textbf{Instruments}: BTC and ETH 15-minute binary options (up/down markets)
\item \textbf{Period}: July 2024 - September 2025
\item \textbf{Observations}: 39 million predictions on test set
\item \textbf{Data sources}: Polymarket CLOB, Deribit (for IV benchmarks), Polygon blockchain
\end{itemize}
\subsection{Overall Performance}
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Black-Scholes} & \textbf{BS + ML} \\
\midrule
Brier Score & 0.1615 & 0.1524 \\
Improvement & — & 5.6\% \\
MAE & 0.2881 & 0.2771 \\
Win Rate & 50.10\% & 50.19\% \\
\bottomrule
\end{tabular}
Test set performance metrics. Brier score reduction of 5.6\% corresponds to $\sqrt{0.0091} = 0.095$ or 9.5 basis points lower root-mean-squared probability error.\\\rule{\textwidth}{0.4pt}
\label{tab:overall_performance}
\end{table}
\subsection{Delta and Gamma Analysis}
Binary options exhibit extreme non-linearity. Empirical delta (computed via finite differences) varies from near-zero deep out-of-the-money to 1.91 deep in-the-money, a range of 138× (coefficient of variation = 137.6\%). This contrasts with vanilla call options where delta is bounded [0, 1] with typical CV around 30-40\%.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/01_delta_surface_2d.png}
Empirical delta surface compared to Black-Scholes theoretical delta. The model captures smoother transitions than theory, suggesting market participants incorporate information beyond simple log-normal dynamics.\\\rule{\textwidth}{0.4pt}
\label{fig:delta}
\end{figure}
Gamma (second derivative) peaks at at-the-money strikes with values up to 3.50, indicating extreme sensitivity to small price movements. This convexity hotspot explains why ATM options have highest prediction difficulty (Section \ref{sec:error_analysis}).
\subsection{Feature Importance}
Correlation analysis between features and LightGBM residual predictions reveals:
\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Feature} & \textbf{Correlation with Residuals} \\
\midrule
Moneyness & 41.0\% \\
Momentum 300s & 32.0\% \\
Momentum 900s & 22.0\% \\
Momentum 60s & 19.0\% \\
RV 3600s & 5.6\% \\
Time remaining & 4.2\% \\
Autocorrelation lag-1 & 3.8\% \\
\bottomrule
\end{tabular}
Top 7 features by correlation with residual predictions. Moneyness and momentum dominate, suggesting corrections primarily adjust for directional bias not captured by Black-Scholes.\\\rule{\textwidth}{0.4pt}
\label{tab:feature_importance}
\end{table}
\subsection{Regime Analysis}
\label{sec:regime_analysis}
K-means clustering on five key features (moneyness, RV\_900s, time\_remaining, jump\_intensity\_300s, autocorr\_lag1\_300s) identifies five distinct market regimes with heterogeneous performance:
\begin{table}[h]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Regime} & \textbf{Data \%} & \textbf{MAE BS} & \textbf{MAE ML} & \textbf{Improv.} & \textbf{Characteristics} \\
\midrule
3 & 35.0\% & 0.2068 & 0.2068 & 24.8\% & ATM, low vol, mid-expiry \\
4 & 17.7\% & 0.3184 & 0.3629 & 12.2\% & ATM, low vol, long TTL, high autocorr \\
1 & 6.6\% & 0.2387 & 0.2529 & 5.6\% & ITM, high vol, long TTL \\
0 & 6.6\% & 0.2344 & 0.2456 & 4.5\% & OTM, high vol, long TTL \\
2 & 34.4\% & 0.4261 & 0.4546 & 6.3\% & ATM, extreme TTL (very long) \\
\bottomrule
\end{tabular}
Performance by feature regime. Regime 3 (best regime) accounts for 35\% of data and achieves 24.8\% Brier improvement. Regime 2 has poor absolute performance but still shows modest improvement.\\\rule{\textwidth}{0.4pt}
\label{tab:regimes}
\end{table}
\textbf{Key insights}:
\begin{itemize}
\item \textbf{Regime 3}: Optimal conditions for ML corrections—ATM options with low realized volatility and medium expiry (200-400 seconds). Model achieves 24.8\% improvement here.
\item \textbf{Regime 4}: Long-dated ATM options with high autocorrelation. Persistent trends allow momentum features to add value (12.2\% improvement).
\item \textbf{Regimes 0 \& 1}: High-volatility, long-dated options far from strike. Large jumps and volatility clustering make corrections difficult (4-6\% improvement).
\item \textbf{Regime 2}: Extreme time-to-expiry options (very long dated). Poor baseline performance (MAE = 0.426) limits correction potential despite 6.3\% relative improvement.
\end{itemize}
\subsection{Error Analysis}
\label{sec:error_analysis}
Conditional error analysis reveals systematic patterns:
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Condition} & \textbf{MAE Baseline} & \textbf{MAE ML (Improvement)} \\
\midrule
No Jump & 0.2801 & 0.2699 (3.6\%) \\
Jump Detected & 0.3421 & 0.3298 (3.6\%) \\
Low Volatility & 0.2156 & 0.1998 (7.3\%) \\
High Volatility & 0.3794 & 0.3689 (2.8\%) \\
Fresh IV ($<$60s) & 0.2843 & 0.2734 (3.8\%) \\
Stale IV ($>$300s) & 0.3012 & 0.2901 (3.7\%) \\
\bottomrule
\end{tabular}
Conditional error analysis. Model performs best in low-volatility regimes (7.3\% improvement) and struggles in high-volatility environments (2.8\%).\\\rule{\textwidth}{0.4pt}
\label{tab:conditional_errors}
\end{table}
Error heatmaps (Figure \ref{fig:error_heatmap}) show worst predictions occur at:
\begin{itemize}
\item ATM + high volatility + short time-to-expiry ($<$120s)
\item Deep OTM + volatility spikes
\item Extreme moneyness ($|m| > 2\%$) + low liquidity
\end{itemize}
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/09_error_heatmap_2d.png}
2D error heatmap (moneyness × realized volatility). Darkest regions indicate highest MAE. Worst errors cluster at ATM + high volatility, where gamma is highest and volatility estimation is most critical.\\\rule{\textwidth}{0.4pt}
\label{fig:error_heatmap}
\end{figure}
\subsection{Calibration and Win Rate Analysis}
Model predictions are well-calibrated across most probability ranges:
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/15_win_rate_analysis.png}
Calibration plot showing actual vs predicted win rates. ML model (green) tracks ideal 45° line more closely than Black-Scholes baseline (red), particularly in the [0.4, 0.6] probability range. Slight overconfidence appears at extremes ($P < 0.1$ and $P > 0.9$).\\\rule{\textwidth}{0.4pt}
\label{fig:calibration}
\end{figure}
Overall win rate: 50.19\% (nearly perfect for binary options where true probability should center at 50\%).
\subsection{Trading Signal Analysis}
We define a trading signal as taking a position when $|P_{\text{final}} - P_{\text{market}}| > \theta$ (edge threshold). Signal quality varies with threshold:
\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Edge Threshold} & \textbf{Signal Count} & \textbf{Win Rate} & \textbf{Opportunity \%} \\
\midrule
1\% & 887,339 & 74.3\% & 45.8\% \\
2\% & 831,507 & 75.0\% & 42.9\% \\
3\% & 777,692 & 75.9\% & 40.1\% \\
5\% & 659,417 & 77.5\% & 34.0\% \\
10\% & 340,525 & 83.5\% & 17.6\% \\
\bottomrule
\end{tabular}
Trading signal quality by edge threshold. 10\% edge achieves 83.5\% win rate but only occurs in 17.6\% of observations, creating a natural selectivity-frequency tradeoff.\\\rule{\textwidth}{0.4pt}
\label{tab:signals}
\end{table}
\textbf{Practical implications}:
\begin{itemize}
\item \textbf{High-frequency strategy}: Use 2-3\% thresholds for 40\% opportunity rate with 75\% win rate
\item \textbf{Selective strategy}: Use 10\% threshold for 83.5\% win rate on 17.6\% of opportunities
\item \textbf{Regime-based}: Increase thresholds in Regime 2 (poor baseline), decrease in Regime 3 (optimal conditions)
\end{itemize}
\section{Binary Payoff Implications for Machine Learning}
The discontinuous payoff structure of binary options creates five key differences from vanilla option pricing:
\subsection{Non-Linear Baseline is Essential}
Linear models achieve $R^2 = 0.75$ when predicting binary outcomes directly. Black-Scholes baseline achieves $R^2 = 0.99$. The sigmoid shape of $\Phi(d_2)$ is critical for capturing the probability transition from 0 to 1 as moneyness varies.
\subsection{Heteroskedastic Error Structure}
Vanilla options have roughly constant error variance across strikes (proportional to vega). Binary options have maximum error variance at ATM:
\begin{equation}
\text{Var}(\epsilon | m) \approx P_{\text{BS}}(m) \cdot (1 - P_{\text{BS}}(m))
\end{equation}
This suggests weighted loss functions (Section \ref{sec:future_work}) could improve performance.
\subsection{Residual Learning is Necessary}
Direct probability prediction with ML (without Black-Scholes baseline) performs poorly:
\begin{itemize}
\item Direct ML: Brier = 0.1893
\item BS only: Brier = 0.1615
\item BS + ML: Brier = 0.1524
\end{itemize}
The non-linear baseline captures the structural relationship between moneyness and probability, allowing ML to focus on deviations from theory.
\subsection{Feature Engineering Focuses on Discrete Dynamics}
Unlike vanilla options where vega dominates, binary options are most sensitive to directional moves near strikes. This explains why momentum features (32\%, 22\%, 19\% correlation) matter more than volatility features (5.6\%).
\subsection{Direct Loss-to-Metric Connection}
Theorem 1 (Section 2.3) shows that MSE of residuals equals Brier score improvement. This direct connection doesn't hold for vanilla options where pricing errors compound non-linearly through Black-Scholes formula.
\section{Future Work}
\label{sec:future_work}
We propose five complementary improvements to the current architecture, ordered by expected return-on-investment and implementation complexity.
\subsection{Advanced Moneyness Features}
\subsubsection{Motivation}
Current model uses simple moneyness $(S-K)/K$, which exhibits 41\% correlation with residuals—the highest of all 196 features. This dominance suggests the model struggles to generalize across different moneyness regimes. The non-symmetric nature of simple ratios (e.g., S/K = 0.99 vs 1.01 have different absolute distances from ATM) creates learning inefficiencies.
\subsubsection{Proposed Features}
\textbf{Priority 1: Log-Moneyness}
\begin{equation}
m_{\text{log}} = \ln(S/K)
\end{equation}
Properties: Symmetric around ATM ($m_{\text{log}} = 0$ when $S=K$), unbounded range $(-\infty, \infty)$, linear relationship with percentage price moves.
\textbf{Priority 2: Standardized Moneyness}
\begin{equation}
m_{\text{std}} = \frac{\ln(S/K)}{\sigma\sqrt{T}}
\end{equation}
Properties: Normalizes distance-to-strike by volatility and time, measures "standard deviations from ATM", collapses different vol/time regimes into comparable units. This is essentially $d_2$ from Black-Scholes without the drift term.
\textbf{Priority 3: Moneyness Squared}
\begin{equation}
m_{\text{sq}} = [\ln(S/K)]^2
\end{equation}
Properties: Captures non-linear tail effects, symmetric parabola around ATM.
\subsubsection{Implementation}
\begin{center}\textbf{Algorithm:} 
Adding Advanced Moneyness Features\\\rule{\textwidth}{0.4pt}
\begin{enumerate}
\item Convert time\_remaining to years: $T_{\text{years}} = \text{time\_remaining} / (365.25 \times 24 \times 3600)$
\item Compute log-moneyness: $m_{\text{log}} = \ln(S/K)$
\item Compute standardized moneyness: $m_{\text{std}} = m_{\text{log}} / (\sigma \sqrt{T_{\text{years}}})$
\item Compute moneyness squared: $m_{\text{sq}} = m_{\text{log}}^2$
\item Add interaction terms: $m_{\text{log}} \times T$, $m_{\text{log}} \times \sigma$
\item Update feature list (5 new features added)
\item Retrain LightGBM model
\item Evaluate residual correlation reduction
\end{enumerate}
\end{center}
\subsubsection{Expected Outcomes}
\begin{itemize}
\item \textbf{Residual correlation}: 41\% $\to$ 25-30\% (10-15 percentage point reduction)
\item \textbf{Brier improvement}: Additional 2-4\% reduction (0.1524 $\to$ 0.1460-0.1490)
\item \textbf{Regime generalization}: Better performance across vol/time regimes
\item \textbf{Implementation effort}: 2-3 hours
\item \textbf{Risk}: Low (additive features, no breaking changes)
\end{itemize}
\subsection{Weighted MSE for Heteroskedastic Errors}
\subsubsection{Theoretical Justification}
Binary outcome variance follows:
\begin{equation}
\text{Var}(Y | P) = P(1-P)
\end{equation}
This creates heteroskedastic errors with maximum variance at $P=0.5$ (ATM) and minimum at extremes. Weighted Least Squares theory suggests inverse variance weighting:
\begin{equation}
w_i = \frac{1}{\text{Var}(Y_i | P_i)} = \frac{1}{P_i(1-P_i) + \delta}
\end{equation}
where $\delta \approx 0.01$ ensures numerical stability.
\subsubsection{Loss Function}
\begin{equation}
\mathcal{L}_{\text{weighted}} = \frac{1}{N} \sum_{i=1}^{N} w_i (\epsilon_i - \hat{\epsilon}_i)^2
\end{equation}
Normalized weights (divide by mean) maintain overall scale:
\begin{equation}
w_i^{\text{norm}} = \frac{w_i}{\bar{w}} = \frac{N \cdot w_i}{\sum_{j=1}^{N} w_j}
\end{equation}
\subsubsection{Implementation in LightGBM}
\begin{center}\textbf{Algorithm:} 
Weighted MSE Training\\\rule{\textwidth}{0.4pt}
\begin{enumerate}
\item Load baseline probabilities $P_{\text{BS}}$
\item Clip to numerical range: $P_{\text{clipped}} = \text{clip}(P_{\text{BS}}, 0.01, 0.99)$
\item Compute variance: $v = P_{\text{clipped}} \cdot (1 - P_{\text{clipped}})$
\item Compute weights: $w = 1 / v$
\item Normalize: $w_{\text{norm}} = w / \text{mean}(w)$
\item Create LightGBM dataset with sample weights
\item Train model: \texttt{lgb.train(params, train\_data, weight=w\_norm)}
\item Evaluate on test set (compare weighted vs unweighted)
\end{enumerate}
\end{center}
\subsubsection{Weight Distribution}
Expected weight statistics for your data:
\begin{itemize}
\item At $P=0.01$ or $P=0.99$: $w \approx 101$ (25× baseline)
\item At $P=0.25$ or $P=0.75$: $w \approx 5.3$ (1.3× baseline)
\item At $P=0.5$ (ATM): $w = 4.0$ (baseline)
\end{itemize}
\subsubsection{Expected Outcomes}
\begin{itemize}
\item \textbf{Tail improvement}: 5-10\% Brier reduction for $P<0.1$ and $P>0.9$
\item \textbf{Overall improvement}: Additional 2-4\% Brier reduction
\item \textbf{Trade-off}: Small accuracy loss at ATM ($<$2\%) for large tail gains (30-50\%)
\item \textbf{Implementation effort}: 1 day
\item \textbf{Risk}: Moderate (requires hyperparameter retuning)
\end{itemize}
\subsection{Log-Odds Space Transformation}
\subsubsection{Motivation}
Current approach adds residuals in probability space, causing two issues:
\begin{enumerate}
\item Requires clipping at [0, 1] boundaries (loses information)
\item Slight overconfidence at extremes ($P<0.1$, $P>0.9$) per calibration analysis
\end{enumerate}
Log-odds (logit) space provides natural bounds and better numerical properties for extreme probabilities.
\subsubsection{Mathematical Framework}
\textbf{Forward transformation:}
\begin{equation}
\text{logit}(P) = \ln\left(\frac{P}{1-P}\right), \quad P \in (0,1) \to \mathbb{R}
\end{equation}
\textbf{Inverse transformation:}
\begin{equation}
\text{expit}(z) = \text{sigmoid}(z) = \frac{1}{1 + e^{-z}}, \quad z \in \mathbb{R} \to (0,1)
\end{equation}
\textbf{Proposed residual learning:}
\begin{align}
z_{\text{BS}} &= \text{logit}(P_{\text{BS}}) \\
\hat{\epsilon}_{\text{logit}} &= f_{\text{LightGBM}}(X) \\
P_{\text{final}} &= \text{expit}(z_{\text{BS}} + \hat{\epsilon}_{\text{logit}})
\end{align}
\subsubsection{Residual Target Definition}
Since outcomes $Y \in \{0, 1\}$ cannot be directly transformed to logit space, use gradient-based target:
\begin{equation}
\epsilon_{\text{logit}} = \frac{Y - P_{\text{BS}}}{P_{\text{BS}}(1 - P_{\text{BS}})}
\end{equation}
This is the derivative of Brier score with respect to $\text{logit}(P_{\text{BS}})$. Clip to $[-20, 20]$ for numerical stability.
\subsubsection{Advantages}
\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{Probability Space} & \textbf{Log-Odds Space} \\
\midrule
Extreme values & Compressed near 0/1 & Stretched $(-\infty, \infty)$ \\
Natural bounds & Requires clipping & Automatic via sigmoid \\
Symmetry & Asymmetric tails & Symmetric around 0 \\
Numerical stability & Issues at $P \to 0, 1$ & Stable across range \\
Additivity & Multiplicative effects & Additive effects \\
\bottomrule
\end{tabular}
\end{table}
\subsubsection{Implementation}
\begin{center}\textbf{Algorithm:} 
Log-Odds Residual Learning\\\rule{\textwidth}{0.4pt}
\begin{enumerate}
\item Clip baseline: $P_{\text{clipped}} = \text{clip}(P_{\text{BS}}, 10^{-7}, 1-10^{-7})$
\item Transform to logit: $z_{\text{BS}} = \text{logit}(P_{\text{clipped}})$ (use scipy.special.logit)
\item Compute gradient target: $\epsilon_{\text{logit}} = (Y - P_{\text{BS}}) / (P_{\text{BS}}(1-P_{\text{BS}}))$
\item Clip residuals: $\epsilon_{\text{clipped}} = \text{clip}(\epsilon_{\text{logit}}, -20, 20)$
\item Train LightGBM on $\epsilon_{\text{clipped}}$ as target
\item Predict: $\hat{\epsilon}_{\text{logit}} = \text{model.predict}(X)$
\item Apply correction: $z_{\text{final}} = z_{\text{BS}} + \hat{\epsilon}_{\text{logit}}$
\item Transform back: $P_{\text{final}} = \text{expit}(z_{\text{final}})$ (use scipy.special.expit)
\end{enumerate}
\end{center}
\subsubsection{Expected Outcomes}
\begin{itemize}
\item \textbf{Extreme probability calibration}: 10-20\% improvement for $P<0.05$ and $P>0.95$
\item \textbf{Overall improvement}: Additional 1-2\% Brier reduction
\item \textbf{No clipping loss}: Information preserved at boundaries
\item \textbf{Implementation effort}: 2-3 days
\item \textbf{Risk}: Moderate (new target definition, requires validation)
\end{itemize}
\subsection{Confusion Matrix Bucketed Analysis}
\subsubsection{Motivation}
Current evaluation focuses on probabilistic metrics (Brier, log-loss, calibration). For trading decisions, we need classification metrics (precision, recall, F1) to understand:
\begin{itemize}
\item Where should we trade? (high precision/recall buckets)
\item Where should we avoid? (uncertain zone $P \in [0.45, 0.55]$)
\item How do BS baseline and ML model differ?
\end{itemize}
\subsubsection{Methodology}
\textbf{Bucket Strategy}: Use 10 equal-width bins: [0-0.1), [0.1-0.2), ..., [0.9-1.0]
For each bucket $b$:
\begin{enumerate}
\item Filter predictions: $\mathcal{D}_b = \{(y_i, \hat{p}_i) : \hat{p}_i \in [b_{\text{min}}, b_{\text{max}})\}$
\item Apply decision threshold: $\hat{y}_i = \mathbb{I}[\hat{p}_i \geq 0.5]$
\item Compute confusion matrix:
\[
\text{CM}_b = \begin{bmatrix}
\text{TN}_b & \text{FP}_b \\
\text{FN}_b & \text{TP}_b
\end{bmatrix}
\]
\item Calculate metrics:
\begin{align}
\text{Precision}_b &= \frac{\text{TP}_b}{\text{TP}_b + \text{FP}_b} \\
\text{Recall}_b &= \frac{\text{TP}_b}{\text{TP}_b + \text{FN}_b} \\
\text{F1}_b &= \frac{2 \cdot \text{Precision}_b \cdot \text{Recall}_b}{\text{Precision}_b + \text{Recall}_b}
\end{align}
\item Compute probabilistic metrics: Brier$_b$, LogLoss$_b$, Calibration Error$_b$
\end{enumerate}
\subsubsection{Comparison Framework}
Run analysis independently for:
\begin{itemize}
\item BS baseline: bucket by $P_{\text{BS}}$, compute metrics
\item ML model: bucket by $P_{\text{final}}$, compute metrics
\item Compute improvement: $\Delta_b = \text{Metric}_{\text{ML},b} - \text{Metric}_{\text{BS},b}$
\item Statistical significance: paired t-test per bucket
\end{itemize}
\subsubsection{Trading Insights}
Expected bucket characteristics:
\begin{table}[h]
\centering
\small
\begin{tabular}{lp{6cm}}
\toprule
\textbf{Bucket} & \textbf{Expected Behavior} \\
\midrule
{[}0.0-0.3{]} & Strong sell signal: High specificity, low FPR, good for shorting \\
{[}0.3-0.45{]} & Weak sell: Moderate precision, higher FN rate \\
{[}0.45-0.55{]} & Uncertain zone: Low precision/recall, AVOID TRADING \\
{[}0.55-0.7{]} & Weak buy: Moderate recall, higher FP rate \\
{[}0.7-1.0{]} & Strong buy signal: High recall, low FNR, good for longing \\
\bottomrule
\end{tabular}
\end{table}
\subsubsection{Implementation}
\begin{center}\textbf{Algorithm:} 
Bucketed Confusion Matrix Analysis\\\rule{\textwidth}{0.4pt}
\begin{enumerate}
\item Define buckets: $B = \{[0.0, 0.1), [0.1, 0.2), \ldots, [0.9, 1.0]\}$
\item For each model $M \in \{\text{BS}, \text{ML}\}$:
    \begin{enumerate}
    \item For each bucket $b \in B$:
        \begin{enumerate}
        \item Filter: $\mathcal{D}_b = \{i : P_{M,i} \in b\}$
        \item Predict: $\hat{y}_i = \mathbb{I}[P_{M,i} \geq 0.5]$
        \item Compute CM$_b$: confusion\_matrix($y_{\mathcal{D}_b}$, $\hat{y}_{\mathcal{D}_b}$)
        \item Compute metrics: Precision$_b$, Recall$_b$, F1$_b$, Brier$_b$
        \end{enumerate}
    \end{enumerate}

\item Compute improvements: $\Delta_b = \text{ML}_b - \text{BS}_b$ for all metrics
\item Visualize: 4-panel plot (Precision, Recall, F1, Brier) vs bucket
\item Statistical tests: paired t-test per bucket for Brier$_b$
\end{enumerate}
\end{center}
\subsubsection{Expected Outcomes}
\begin{itemize}
\item \textbf{Trading zones identified}: Clear separation of high-confidence vs uncertain regions
\item \textbf{Model comparison}: Quantify where ML outperforms BS (likely mid-range [0.3-0.7])
\item \textbf{Risk management}: Avoid [0.45-0.55] bucket (high error rate)
\item \textbf{Implementation effort}: 1 day
\item \textbf{Risk}: Low (evaluation only, no model changes)
\end{itemize}
\subsection{Feedforward Neural Network Architecture}
\subsubsection{Literature Context}
Recent research (2023-2024) on tabular deep learning shows:
\begin{itemize}
\item Gradient boosted trees (XGBoost/LightGBM) outperform neural networks on most tabular benchmarks
\item Neural networks excel on unstructured data (images, text) but struggle with irregular tabular functions
\item \textbf{Ensemble approaches} (GBDT + NN) show 1-3\% improvement over GBDT alone
\end{itemize}
For your use case (196 engineered features, tabular structure, 63M rows), LightGBM is likely optimal. Neural networks are worth exploring for \textbf{ensemble gains} only.
\subsubsection{Proposed Architecture}
\textbf{Simple Feedforward Network:}
\begin{align}
h_1 &= \text{ReLU}(W_1 X + b_1), \quad W_1 \in \mathbb{R}^{512 \times 196} \\
h_1' &= \text{Dropout}(h_1, p=0.3) \\
h_2 &= \text{ReLU}(W_2 h_1' + b_2), \quad W_2 \in \mathbb{R}^{256 \times 512} \\
h_2' &= \text{Dropout}(h_2, p=0.3) \\
h_3 &= \text{ReLU}(W_3 h_2' + b_3), \quad W_3 \in \mathbb{R}^{128 \times 256} \\
\hat{\epsilon} &= W_4 h_3 + b_4, \quad W_4 \in \mathbb{R}^{1 \times 128}
\end{align}
\textbf{Training Configuration:}
\begin{itemize}
\item Optimizer: AdamW with weight decay $10^{-5}$
\item Learning rate: $10^{-3}$ with cosine annealing
\item Batch size: 4,096 (balance between stability and regularization)
\item Epochs: 50-200 with early stopping (patience=15)
\item Loss: MSE or weighted MSE
\end{itemize}
\subsubsection{Ensemble Strategy}
Rather than replacing LightGBM, use ensemble:
\textbf{Option 1: Weighted Average}
\begin{equation}
P_{\text{final}} = P_{\text{BS}} + \alpha \cdot \hat{\epsilon}_{\text{LGB}} + (1-\alpha) \cdot \hat{\epsilon}_{\text{NN}}
\end{equation}
Tune $\alpha \in [0.5, 0.8]$ on validation set.
\textbf{Option 2: Stacking}
\begin{equation}
P_{\text{final}} = P_{\text{BS}} + \text{Ridge}([\hat{\epsilon}_{\text{LGB}}, \hat{\epsilon}_{\text{NN}}])
\end{equation}
Train meta-model (Ridge regression) on held-out predictions.
\subsubsection{Expected Outcomes}
\begin{itemize}
\item \textbf{NN alone}: Likely 3-5\% improvement (worse than LightGBM's 5.6\%)
\item \textbf{Ensemble}: Potentially 6-8\% improvement (additional 0.5-2\% over LightGBM)
\item \textbf{Trade-offs}: 10× slower training, complex deployment, harder tuning
\item \textbf{Implementation effort}: 1 week (including architecture search)
\item \textbf{Risk}: High (uncertain ROI, significant effort)
\item \textbf{Recommendation}: \textbf{Lowest priority}—only pursue if LightGBM plateaus
\end{itemize}
\subsection{Implementation Roadmap}
\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Enhancement} & \textbf{Effort} & \textbf{Expected Gain} & \textbf{Risk} & \textbf{Priority} \\
\midrule
Advanced Moneyness & 2-3 hours & 2-4\% Brier & Low & 1 \\
Weighted MSE & 1 day & 2-4\% Brier & Moderate & 2 \\
Confusion Matrix & 1 day & Trading insights & Low & 3 \\
Log-Odds Transform & 2-3 days & 1-2\% Brier & Moderate & 4 \\
Neural Networks & 1 week & 0-2\% Brier & High & 5 \\
\bottomrule
\end{tabular}
Implementation roadmap ordered by return-on-investment.\\\rule{\textwidth}{0.4pt}
\end{table}
\textbf{Recommended Sequence:}
\begin{enumerate}
\item Start with advanced moneyness features (highest ROI, lowest risk)
\item Implement weighted MSE if moneyness features succeed
\item Run confusion matrix analysis to understand trading zones
\item Evaluate log-odds transformation based on calibration needs
\item Skip neural networks unless overall Brier improvement plateaus below target
\end{enumerate}
\textbf{Cumulative Expected Improvement:} Phases 1-3 should yield 7-14\% total Brier reduction (current 5.6\% + additional 2-8\%), bringing final Brier score to approximately 0.140-0.148 (vs current 0.1524).
\section{Conclusion}
We present a two-stage residual learning framework for binary option pricing that achieves 5.6\% Brier score improvement over Black-Scholes baseline on 39 million predictions. Our approach combines theoretical pricing with data-driven corrections, leveraging 196 engineered features to capture deviations from geometric Brownian motion assumptions.
Key findings include:
\begin{enumerate}
\item \textbf{Theoretical}: MSE of residuals directly equals Brier score improvement, providing principled optimization target.
\item \textbf{Empirical}: Performance varies 4.5\% to 24.8\% across five market regimes, with best results in ATM + low-volatility + medium-expiry conditions.
\item \textbf{Features}: Moneyness (41\%) and momentum (32\%, 22\%, 19\%) dominate corrections, reflecting directional bias not captured by theory.
\item \textbf{Trading}: 10\% edge threshold achieves 83.5\% win rate on 17.6\% of opportunities, enabling selective high-conviction strategies.
\end{enumerate}
The discontinuous payoff structure of binary options necessitates non-linear baseline modeling and regime-specific feature engineering. Unlike vanilla options where vega dominates, binary option pricing is most sensitive to directional momentum near strikes.
Future work on weighted loss functions, log-odds transformations, neural network architectures, and advanced moneyness features may yield additional 5-15\% Brier improvements, particularly in tail probability regimes and extreme volatility environments.
\section*{Data and Code Availability}
Analysis code, feature engineering pipelines, and model checkpoints are available at: \\
\texttt{/Users/lgierhake/Documents/ETH/BT/research/model/}
\begin{itemize}
\item \textbf{Pricing code}: \texttt{01\_pricing/}
\item \textbf{Initial EDA}: \texttt{02\_analysis/}
\item \textbf{Deep EDA}: \texttt{03\_deep\_eda/}
\item \textbf{Feature generation}: \texttt{00\_data\_processing/}
\end{itemize}
\bibliographystyle{plainnat}
\begin{thebibliography}{9}
\bibitem{merton1973theory}
Merton, Robert C.
\textit{Theory of rational option pricing.}
The Bell Journal of Economics and Management Science, 4(1):141-183, 1973.
\bibitem{dugas2009incorporating}
Dugas, Charles, et al.
\textit{Incorporating functional knowledge in neural networks.}
Journal of Machine Learning Research, 10:1239-1262, 2009.
\bibitem{horvath2021deep}
Horv{\'a}th, Blanka, et al.
\textit{Deep learning volatility: A deep neural network perspective on pricing and calibration in (rough) volatility models.}
Quantitative Finance, 21(1):11-27, 2021.
\bibitem{ke2017lightgbm}
Ke, Guolin, et al.
\textit{LightGBM: A highly efficient gradient boosting decision tree.}
Advances in Neural Information Processing Systems, 30:3146-3154, 2017.
\bibitem{wiki:moneyness}
Wikipedia contributors.
\textit{Moneyness --- Wikipedia, The Free Encyclopedia.}
\url{https://en.wikipedia.org/wiki/Moneyness}, 2024. [Online; accessed 11-November-2025].
\end{thebibliography}
\end{document}
