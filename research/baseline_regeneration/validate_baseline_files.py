#!/usr/bin/env python3
"""
Validation script for baseline data files.

Checks file existence, sizes, row counts, and data quality metrics
for all files generated by the baseline regeneration pipeline.

Usage:
    uv run python validate_baseline_files.py [--stage N]

Options:
    --stage N    Validate only files from stage N (1-4)
                 If omitted, validates all files
"""

import sys
from pathlib import Path
from typing import Any

import polars as pl


def format_size(size_bytes: int) -> str:
    """Format file size in human-readable format."""
    for unit in ["B", "KB", "MB", "GB"]:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} TB"


def validate_contract_schedule(file_path: Path) -> dict[str, Any]:
    """Validate contract schedule file (Stage 1)."""
    print("\n" + "=" * 80)
    print("STAGE 1: Contract Schedule Validation")
    print("=" * 80)

    if not file_path.exists():
        print(f"❌ File not found: {file_path}")
        return {"status": "missing", "errors": ["File does not exist"]}

    # File size
    size = file_path.stat().st_size
    print(f"✓ File exists: {file_path}")
    print(f"  Size: {format_size(size)}")

    # Load and validate
    try:
        df = pl.read_parquet(file_path)
        row_count = len(df)
        print(f"  Rows: {row_count:,}")

        # Expected: 70,081 contracts (96 per day * 730 days)
        expected_rows = 70_081
        if abs(row_count - expected_rows) > 100:
            print(f"⚠️  Row count {row_count:,} differs from expected {expected_rows:,}")

        # Check schema
        required_cols = ["open_time", "close_time"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"❌ Missing columns: {missing_cols}")
            return {"status": "invalid", "errors": [f"Missing columns: {missing_cols}"]}

        # Date range
        if "open_time" in df.columns and "close_time" in df.columns:
            min_date = df["open_time"].min()
            max_date = df["close_time"].max()
            print(f"  Date range: {min_date} to {max_date}")

        print("✅ Contract schedule validation PASSED")
        return {"status": "valid", "rows": row_count, "size": size}

    except Exception as e:
        print(f"❌ Validation error: {e}")
        return {"status": "error", "errors": [str(e)]}


def validate_btc_perpetual(file_path: Path) -> dict[str, Any]:
    """Validate BTC perpetual futures file (Stage 2)."""
    print("\n" + "=" * 80)
    print("STAGE 2: BTC Perpetual Futures Validation")
    print("=" * 80)

    if not file_path.exists():
        print(f"❌ File not found: {file_path}")
        return {"status": "missing", "errors": ["File does not exist"]}

    # File size
    size = file_path.stat().st_size
    print(f"✓ File exists: {file_path}")
    print(f"  Size: {format_size(size)}")

    # Expected: ~300-500 MB
    if size < 200_000_000:
        print(f"⚠️  File size {format_size(size)} is smaller than expected (300-500 MB)")

    # Load and validate using lazy evaluation (file is large)
    try:
        df = pl.scan_parquet(file_path)

        # Get row count
        row_count = df.select(pl.len()).collect().item()
        print(f"  Rows: {row_count:,}")

        # Expected: ~63M rows (1-second intervals for 730 days)
        expected_rows = 63_072_000
        if abs(row_count - expected_rows) > 1_000_000:
            print(f"⚠️  Row count {row_count:,} differs from expected ~{expected_rows:,}")

        # Check schema
        stats = df.select([
            pl.col("vwap").mean().alias("avg_price"),
            pl.col("vwap").min().alias("min_price"),
            pl.col("vwap").max().alias("max_price"),
            pl.col("vwap").null_count().alias("null_count"),
        ]).collect()

        avg_price = stats["avg_price"][0]
        min_price = stats["min_price"][0]
        max_price = stats["max_price"][0]
        null_count = stats["null_count"][0]

        print(f"  Average BTC price: ${avg_price:,.2f}")
        print(f"  Price range: ${min_price:,.2f} - ${max_price:,.2f}")
        print(f"  Null values: {null_count:,}")

        # Sanity checks
        if avg_price < 20_000 or avg_price > 100_000:
            print(f"⚠️  Average price ${avg_price:,.2f} is outside expected range (20k-100k)")

        if null_count > row_count * 0.01:
            print(f"⚠️  High null rate: {null_count / row_count * 100:.2f}%")

        print("✅ BTC perpetual validation PASSED")
        return {"status": "valid", "rows": row_count, "size": size}

    except Exception as e:
        print(f"❌ Validation error: {e}")
        return {"status": "error", "errors": [str(e)]}


def validate_risk_free_rates(file_path: Path) -> dict[str, Any]:
    """Validate risk-free rates file (Stage 3)."""
    print("\n" + "=" * 80)
    print("STAGE 3: Risk-Free Rates Validation")
    print("=" * 80)

    if not file_path.exists():
        print(f"❌ File not found: {file_path}")
        return {"status": "missing", "errors": ["File does not exist"]}

    # File size
    size = file_path.stat().st_size
    print(f"✓ File exists: {file_path}")
    print(f"  Size: {format_size(size)}")

    # Expected: ~50 KB
    if size < 10_000:
        print(f"⚠️  File size {format_size(size)} is smaller than expected (~50 KB)")

    # Load and validate
    try:
        df = pl.read_parquet(file_path)
        row_count = len(df)
        print(f"  Rows: {row_count:,}")

        # Expected: 731 daily records
        expected_rows = 731
        if abs(row_count - expected_rows) > 10:
            print(f"⚠️  Row count {row_count:,} differs from expected {expected_rows:,}")

        # Check schema
        if "blended_supply_apr" in df.columns:
            avg_apr = df["blended_supply_apr"].mean()
            min_apr = df["blended_supply_apr"].min()
            max_apr = df["blended_supply_apr"].max()
            null_count = df["blended_supply_apr"].null_count()

            print(f"  Average APR: {avg_apr:.4f} ({avg_apr * 100:.2f}%)")
            print(f"  APR range: {min_apr:.4f} - {max_apr:.4f}")
            print(f"  Null values: {null_count:,}")

            # Sanity checks
            if avg_apr < 0.01 or avg_apr > 0.20:
                print(f"⚠️  Average APR {avg_apr * 100:.2f}% is outside expected range (1-20%)")

        print("✅ Risk-free rates validation PASSED")
        return {"status": "valid", "rows": row_count, "size": size}

    except Exception as e:
        print(f"❌ Validation error: {e}")
        return {"status": "error", "errors": [str(e)]}


def validate_btc_options_iv(file_path: Path) -> dict[str, Any]:
    """Validate BTC options with IV file (Stage 4)."""
    print("\n" + "=" * 80)
    print("STAGE 4: BTC Options + IV Validation")
    print("=" * 80)

    if not file_path.exists():
        print(f"❌ File not found: {file_path}")
        return {"status": "missing", "errors": ["File does not exist"]}

    # File size
    size = file_path.stat().st_size
    print(f"✓ File exists: {file_path}")
    print(f"  Size: {format_size(size)}")

    # Expected: ~500MB-2GB
    if size < 400_000_000:
        print(f"⚠️  File size {format_size(size)} is smaller than expected (500MB-2GB)")

    # Load and validate using lazy evaluation (file is large)
    try:
        df = pl.scan_parquet(file_path)

        # Get statistics
        stats = df.select([
            pl.len().alias("rows"),
            pl.col("implied_vol_mid").null_count().alias("iv_nulls"),
            pl.col("implied_vol_mid").mean().alias("avg_iv"),
            pl.col("implied_vol_mid").min().alias("min_iv"),
            pl.col("implied_vol_mid").max().alias("max_iv"),
        ]).collect()

        row_count = stats["rows"][0]
        iv_nulls = stats["iv_nulls"][0]
        avg_iv = stats["avg_iv"][0]
        min_iv = stats["min_iv"][0]
        max_iv = stats["max_iv"][0]

        print(f"  Rows: {row_count:,}")

        # Expected: 2-10M rows (ATM short-dated options)
        if row_count < 1_000_000:
            print(f"⚠️  Row count {row_count:,} is lower than expected (2-10M)")

        # IV statistics
        null_rate = iv_nulls / row_count * 100
        print(f"  IV nulls: {iv_nulls:,} ({null_rate:.2f}%)")
        print(f"  Average IV: {avg_iv:.4f} ({avg_iv * 100:.1f}%)")
        print(f"  IV range: {min_iv:.4f} - {max_iv:.4f}")

        # Sanity checks
        if null_rate > 1.0:
            print(f"⚠️  High IV null rate: {null_rate:.2f}% (target: <1%)")

        if avg_iv < 0.20 or avg_iv > 1.50:
            print(f"⚠️  Average IV {avg_iv * 100:.1f}% is outside expected range (20-150%)")

        print("✅ BTC options + IV validation PASSED")
        return {"status": "valid", "rows": row_count, "size": size}

    except Exception as e:
        print(f"❌ Validation error: {e}")
        return {"status": "error", "errors": [str(e)]}


def main() -> int:
    """Main validation function."""
    # Parse arguments
    stage = None
    if len(sys.argv) > 1:
        if sys.argv[1] == "--stage" and len(sys.argv) > 2:
            try:
                stage = int(sys.argv[2])
                if stage not in [1, 2, 3, 4]:
                    print("Error: Stage must be 1, 2, 3, or 4")
                    return 1
            except ValueError:
                print("Error: Invalid stage number")
                return 1

    # Define file paths
    project_root = Path(__file__).parent
    files = {
        1: project_root / "research/model/results/contract_schedule.parquet",
        2: project_root / "research/tardis/data/consolidated/btc_perpetual_1s_resampled.parquet",
        3: project_root / "research/risk_free_rate/data/blended_lending_rates_2023_2025.parquet",
        4: project_root / "research/tardis/data/consolidated/btc_options_atm_shortdated_with_iv_2023_2025.parquet",
    }

    validators = {
        1: validate_contract_schedule,
        2: validate_btc_perpetual,
        3: validate_risk_free_rates,
        4: validate_btc_options_iv,
    }

    print("=" * 80)
    print("BASELINE DATA FILES VALIDATION")
    print("=" * 80)

    # Validate specified stage or all stages
    results = {}
    if stage:
        results[stage] = validators[stage](files[stage])
    else:
        for stage_num in [1, 2, 3, 4]:
            results[stage_num] = validators[stage_num](files[stage_num])

    # Summary
    print("\n" + "=" * 80)
    print("VALIDATION SUMMARY")
    print("=" * 80)

    all_valid = True
    for stage_num, result in results.items():
        status = result["status"]
        stage_names = {
            1: "Contract Schedule",
            2: "BTC Perpetual",
            3: "Risk-Free Rates",
            4: "BTC Options + IV",
        }
        if status == "valid":
            print(f"✅ Stage {stage_num} ({stage_names[stage_num]}): VALID")
        elif status == "missing":
            print(f"❌ Stage {stage_num} ({stage_names[stage_num]}): MISSING")
            all_valid = False
        else:
            print(f"❌ Stage {stage_num} ({stage_names[stage_num]}): ERROR")
            all_valid = False

    if all_valid:
        print("\n✅ All validated files are in good condition")
        return 0
    else:
        print("\n⚠️  Some files are missing or invalid")
        return 1


if __name__ == "__main__":
    sys.exit(main())
